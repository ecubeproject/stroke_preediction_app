{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0999821-22c6-4d0d-a07b-b0c96f675e96",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "### According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths (annual mortality rate of 5.5 million).\n",
    "### Every year, more than 15 million people worldwide have a stroke, and in every 4 minutes, someone dies due to stroke.\n",
    "### Stroke is preventable in around 80% of cases. However, for prevention we need to find patients at risk i.e. patients with high probability of stroke.\n",
    "### Here we use machine learning to build a predictive model that predicts the probability of a patient getting stroke based on features like gender, age, various diseases, and smoking status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d859a1-6d1d-4270-b715-04297fe15b1e",
   "metadata": {},
   "source": [
    "# I thought of sharing some domain related resources for the assignment as below. Stroke is a very major disease that has a long run physical and mental impact on the patient and often requires immediate life-saving support and operative treatment methods with a long run rehabilitation. It is a life-threatening disease if not treated on time. Stroke occurs when the blood vessels passing blood from the heart to the brain get damaged either due to plaque (Ischemic) or a rupture (hemorrhage). This is a life threatening disease and often paralyses certain body parts for a long time after initial life saving measures.\n",
    "### https://www.mayoclinic.org/diseases-conditions/stroke/symptoms-causes/syc-20350113 -- Mayo clinic article for starters\n",
    "### https://www.webmd.com/stroke/guide/types-stroke -- explains types of stroke, I opine that Ischemic and Hemorrhagic Stroke are the most common types\n",
    "### https://www.cdc.gov/stroke/signs-symptoms/?CDC_AAref_Val=https://www.cdc.gov/stroke/signs_symptoms.htm\n",
    "### https://www.nhlbi.nih.gov/health/stroke/causes -- explains the major causes of stroke, could be of importance for the assignment\n",
    "### https://www.stroke.org/en/about-stroke -- good information from American Stroke Association\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5e191-5352-4d20-bfbd-94051b804791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Statistical Tests\n",
    "from scipy.stats import gaussian_kde, ks_2samp, mannwhitneyu\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Model Selection & Evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, auc, classification_report, confusion_matrix, f1_score, \n",
    "    precision_score, precision_recall_curve, recall_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Visualization Libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import shap\n",
    "\n",
    "# Summary Tools\n",
    "from summarytools import dfSummary\n",
    "\n",
    "# Ignore Warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbf9d9-7547-4df8-9dbb-8e4a1c9c6074",
   "metadata": {},
   "source": [
    "# Data Loading an Basis statistical EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0baf76-26d3-4823-a2d3-8f22a6341744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "print(df.info())\n",
    "print(\"++++++\"*10)\n",
    "print(df.describe())\n",
    "print(\"++++++\"*10)\n",
    "cat_cols = df.select_dtypes(include=['bool', 'object']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    print(f\"Unique values in {col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc07f38-9acf-4cc7-b9d6-6451509f5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afe6fc-24a8-4aa7-87de-352ef8f2ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fe990-e2cf-40ae-96c8-dce5d6c7c76a",
   "metadata": {},
   "source": [
    "## Attribute Information\n",
    "### 1) id: unique identifier\n",
    "### 2) gender: \"Male\", \"Female\" or \"Other\"\n",
    "### 3) age: age of the patient\n",
    "### 4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "### 5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "### 6) ever_married: \"No\" or \"Yes\"\n",
    "### 7) work_type: \"children\", \"Govt_jov\", \"Never worked\", \"Private\" or \"Self-employed\"\n",
    "### 8) Residence_type: \"Rural\" or \"Urban\"\n",
    "### 9) avg_glucose_level: average glucose level in blood\n",
    "### 10) bmi: body mass index\n",
    "### 11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
    "### 12) stroke: 1 if the patient had a stroke or 0 if not\n",
    "## *Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717999e-fa5f-4f31-9b42-68e4fbffe850",
   "metadata": {},
   "source": [
    "# **UNI-VARIATE ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bc23a-9afc-4c9d-a410-cdb868371355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_categorical_data_plotly(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Generates interactive bar charts for categorical columns using Plotly Express.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataset.\n",
    "    categorical_cols (list): List of categorical column names.\n",
    "    \"\"\"\n",
    "    for col in categorical_cols:\n",
    "        category_counts = df[col].value_counts().reset_index()\n",
    "        category_counts.columns = [col, 'count']  # Rename columns properly\n",
    "        \n",
    "        fig = px.bar(category_counts, x=col, y='count', \n",
    "                     title=f'Distribution of {col}', \n",
    "                     labels={col: col, 'count': 'Count'}, \n",
    "                     color=col, text='count')\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "def visualize_numerical_data_plotly(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Generates interactive histograms with KDE and box plots for numerical columns using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataset.\n",
    "    numeric_cols (list): List of numerical column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Histograms with KDE\n",
    "    for col in numeric_cols:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Histogram\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=df[col], \n",
    "            nbinsx=30, \n",
    "            histnorm='probability density', \n",
    "            name=\"Histogram\", \n",
    "            opacity=0.6, \n",
    "            marker=dict(color='blue')\n",
    "        ))\n",
    "\n",
    "        # KDE Curve (Using Gaussian KDE from SciPy)\n",
    "        kde = gaussian_kde(df[col].dropna())  # Drop NaNs before estimating density\n",
    "        x_range = np.linspace(df[col].min(), df[col].max(), 1000)\n",
    "        kde_values = kde(x_range)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_range, \n",
    "            y=kde_values, \n",
    "            mode='lines', \n",
    "            name=\"KDE\", \n",
    "            line=dict(color='red', width=2)\n",
    "        ))\n",
    "\n",
    "        # Figure Layout\n",
    "        fig.update_layout(\n",
    "            title=f'Distribution of {col} with KDE',\n",
    "            xaxis_title=col,\n",
    "            yaxis_title=\"Density\",\n",
    "            barmode=\"overlay\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0f9c5-0161-4a25-a62b-b2e8b2a4ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_box_plots_plotly(df, numeric_cols):\n",
    "    # Box plots for numerical variables (univariate analysis)\n",
    "    for num_col in numeric_cols:\n",
    "        fig = px.box(df.dropna(subset=[num_col]), y=num_col, title=f'Box Plot of {num_col}')\n",
    "        fig.show()\n",
    "def plot_violin_plots(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Plots violin plots for each numerical variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset containing numeric columns.\n",
    "    numeric_cols (list): List of numerical column names.\n",
    "\n",
    "    Returns:\n",
    "    None (Displays violin plots for each numeric variable).\n",
    "    \"\"\"\n",
    "    for num_col in numeric_cols:\n",
    "        fig = px.violin(df, y=num_col, box=True, points=\"all\",\n",
    "                        title=f'Violin Plot of {num_col}')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706118d-f1d8-4c6c-81a0-72d8ba612f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first two categorical columns\n",
    "categorical_cols = ['gender', 'ever_married']\n",
    "# Call the function \n",
    "visualize_categorical_data_plotly(df, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa453e-531a-4511-b2ae-626e119a15fa",
   "metadata": {},
   "source": [
    "# Interpretations for the categorical variables based on univariate analysis:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4636f753-1617-4409-a1cf-14b918e94bbf",
   "metadata": {},
   "source": [
    "# **Interpretation of Categorical Data Visualization**\n",
    "\n",
    "| **Feature**       | **Category** | **Count** | **Observations** |\n",
    "|------------------|------------|---------|----------------|\n",
    "| **Gender**       | Female     | 2994    | Majority of the dataset consists of female participants. |\n",
    "|                  | Male       | 2115    | Male participants are fewer than females but still form a significant proportion. |\n",
    "|                  | Other      | 1       | Only one participant is categorized as \"Other\". |\n",
    "| **Ever Married** | Yes        | 3353    | A large proportion of individuals in the dataset have been married. |\n",
    "|                  | No         | 1757    | Fewer individuals in the dataset have never been married. |\n",
    "\n",
    "# **Key Insights:**\n",
    "##  The dataset has a significantly higher number of females compared to males.\n",
    "##  The \"Other\" gender category is extremely underrepresented, with only one individual.\n",
    "##  Most individuals in the dataset have been married at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2f20d-a759-4d44-a5d1-4e0e78fea949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next two categorical columns\n",
    "categorical_cols = ['work_type', 'Residence_type']\n",
    "# Call the function \n",
    "visualize_categorical_data_plotly(df, categorical_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6252cd8f-8a6e-4eff-b0ec-3b1623e35400",
   "metadata": {},
   "source": [
    "# Interpretation of Categorical Data Visualization\n",
    "\n",
    "| **Feature**         | **Category**       | **Count** | **Observations** |\n",
    "|--------------------|------------------|---------|----------------|\n",
    "| **Work Type**     | Private          | 2925    | Majority of individuals work in the private sector. |\n",
    "|                  | Self-employed     | 819     | A moderate proportion of individuals are self-employed. |\n",
    "|                  | Children          | 687     | Children category represents a notable portion, likely indicating non-working dependents. |\n",
    "|                  | Govt_job          | 657     | A smaller number of individuals are employed in government jobs. |\n",
    "|                  | Never_worked      | 22      | Very few individuals have never worked. |\n",
    "| **Residence Type** | Urban            | 2596    | The dataset has a fairly even distribution between urban and rural residences. |\n",
    "|                  | Rural             | 2514    | Rural residents form almost half of the dataset. |\n",
    "\n",
    "**Key Insights:**\n",
    "- Most individuals in the dataset are employed in the private sector.\n",
    "- A significant portion of the dataset includes children who are not part of the workforce.\n",
    "- The dataset has a balanced distribution between urban and rural residents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04922819-4150-43b0-8f7a-301a1af81da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next two categorical columns\n",
    "categorical_cols = ['smoking_status', 'hypertension']\n",
    "# Call the function \n",
    "visualize_categorical_data_plotly(df, categorical_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7251de71-0b76-4540-a797-4008aa9fd92c",
   "metadata": {},
   "source": [
    "# Interpretation of Categorical Data Visualization\n",
    "\n",
    "| **Feature**          | **Category**         | **Count** | **Observations** |\n",
    "|---------------------|--------------------|---------|----------------|\n",
    "| **Smoking Status**  | Never smoked       | 1892    | Majority of individuals have never smoked. |\n",
    "|                    | Unknown            | 1544    | A significant portion of the dataset has unknown smoking status, which may require handling. |\n",
    "|                    | Formerly smoked    | 885     | A smaller proportion of individuals were former smokers. |\n",
    "|                    | Smokes             | 789     | The smallest group consists of current smokers. |\n",
    "| **Hypertension**    | No (0)             | 4612    | Most individuals in the dataset do not have hypertension. |\n",
    "|                    | Yes (1)            | 498     | A much smaller proportion of individuals have hypertension. |\n",
    "\n",
    "**Key Insights:**\n",
    "- The dataset has a significant portion of individuals with unknown smoking status, which might need imputation or separate category handling.\n",
    "- The majority of individuals have never smoked, while the number of current and former smokers is much lower.\n",
    "- Hypertension is an imbalanced feature, with a large majority of individuals not having the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48715e6-de35-47b0-a633-be0bcc91f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define next two categorical columns\n",
    "categorical_cols = ['heart_disease', 'stroke']\n",
    "# Call the function \n",
    "visualize_categorical_data_plotly(df, categorical_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b84b369-655b-42f7-8bc9-e220cf589bb0",
   "metadata": {},
   "source": [
    "## Interpretation of Categorical Data Visualization\n",
    "\n",
    "| **Feature**       | **Category** | **Count** | **Observations** |\n",
    "|------------------|------------|---------|----------------|\n",
    "| **Heart Disease** | No (0)    | 4834    | Majority of individuals do not have heart disease. |\n",
    "|                  | Yes (1)   | 276     | A small proportion of individuals have heart disease. |\n",
    "| **Stroke**       | No (0)    | 4861    | The dataset is highly imbalanced, with most individuals not having experienced a stroke. |\n",
    "|                  | Yes (1)   | 249     | Only a small fraction of the dataset has had a stroke. |\n",
    "\n",
    "**Key Insights:**\n",
    "- Heart disease is relatively rare in the dataset, affecting only a small portion of individuals.\n",
    "- The dataset is highly imbalanced for stroke prediction, with a significantly lower number of stroke cases (only 249 instances).\n",
    "- This class imbalance may require resampling techniques such as SMOTE or weighted loss functions to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c23a72-aab7-4b36-bb55-ea609d69079e",
   "metadata": {},
   "source": [
    "| **Feature**          | **Category**       | **Count** | **Observations** |\n",
    "|----------------------|-------------------|---------|----------------|\n",
    "| **Gender**          | Female            | 2994    | Majority of the dataset consists of female participants. |\n",
    "|                    | Male              | 2115    | Male participants are fewer than females but still form a significant proportion. |\n",
    "|                    | Other             | 1       | Only one participant is categorized as \"Other\". |\n",
    "| **Ever Married**    | Yes               | 3353    | A large proportion of individuals in the dataset have been married. |\n",
    "|                    | No                | 1757    | Fewer individuals in the dataset have never been married. |\n",
    "| **Work Type**      | Private           | 2925    | Majority of individuals work in the private sector. |\n",
    "|                    | Self-employed     | 819     | A moderate proportion of individuals are self-employed. |\n",
    "|                    | Children          | 687     | Children category represents a notable portion, likely indicating non-working dependents. |\n",
    "|                    | Govt_job          | 657     | A smaller number of individuals are employed in government jobs. |\n",
    "|                    | Never_worked      | 22      | Very few individuals have never worked. |\n",
    "| **Residence Type**  | Urban             | 2596    | The dataset has a fairly even distribution between urban and rural residences. | \n",
    "|                    | Rural             | 2514    | Rural residents form almost half of the dataset. |\n",
    "| **Smoking Status**  | Never smoked      | 1892    | Majority of individuals have never smoked. |\n",
    "|                    | Unknown           | 1544    | A significant portion of the dataset has unknown smoking status, which may require handling. |\n",
    "|                    | Formerly smoked   | 885     | A smaller proportion of individuals were former smokers. |\n",
    "|                    | Smokes            | 789     | The smallest group consists of current smokers. |\n",
    "| **Hypertension**    | No (0)            | 4612    | Most individuals in the dataset do not have hypertension. |\n",
    "|                    | Yes (1)           | 498     | A much smaller proportion of individuals have hypertension. |\n",
    "| **Heart Disease**   | No (0)            | 4834    | Majority of individuals do not have heart disease. |\n",
    "|                    | Yes (1)           | 276     | A small proportion of individuals have heart disease. |\n",
    "| **Stroke**         | No (0)            | 4861    | The dataset is highly imbalanced, with most individuals not having experienced a stroke. |\n",
    "|                    | Yes (1)           | 249     | Only a small fraction of the dataset has had a stroke. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60eb95-9360-46d4-bd40-b48def3fa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns\n",
    "numeric_cols = ['age', 'avg_glucose_level', 'bmi']\n",
    "# Call the function \n",
    "visualize_numerical_data_plotly(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d59ab-91d2-4b3e-ac11-fc79833db144",
   "metadata": {},
   "source": [
    "# Interpretation Of Histograms For Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199216dc-fce9-4111-9e43-d55bdbbc74a8",
   "metadata": {},
   "source": [
    "| **Feature**             | **Observations** |\n",
    "|------------------------|----------------|\n",
    "| **Age**               | The distribution of age appears fairly uniform, with a slight concentration in the middle-aged range. The KDE curve suggests that there is no extreme skewness, but a slight concentration in the middle range. |\n",
    "| **Avg Glucose Level**  | The distribution of average glucose levels is right-skewed, indicating that most individuals have lower glucose levels, but there are some individuals with significantly higher values. The KDE curve confirms this skewness. |\n",
    "| **BMI**               | The BMI distribution is approximately normal but slightly right-skewed. Most individuals have BMI values concentrated around the 20-35 range, with fewer cases at the extreme ends. |\n",
    "\n",
    "**Key Insights:**\n",
    "- Age distribution is fairly uniform, meaning the dataset covers individuals across different age groups.\n",
    "- Average glucose level has a strong right skew, which might require transformation (e.g., log transformation) if modeling is affected.\n",
    "- BMI is roughly normal but has some right skew, indicating the presence of individuals with very high BMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9244fa-e646-4239-86ba-e4d8d953845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_box_plots_plotly(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f9f07-078b-4b3f-93fb-d906e4644897",
   "metadata": {},
   "source": [
    "# Interpretation of Box Plots for Numeric Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "787dc63c-9f94-4e25-8854-2e3ec353dd4f",
   "metadata": {},
   "source": [
    "### **Interpretation of Skewness, Outliers, and Treatment**\n",
    "\n",
    "| Feature | Interpretation (Skewness, Outliers, and Treatment) |\n",
    "|---------|--------------------------------------------------|\n",
    "| **Age** | The box plot shows a **uniformly distributed age range** with no significant skewness. There are **no extreme outliers**, indicating a well-distributed dataset. Since age is a critical predictor for stroke, no transformation is needed. Retaining all values is advisable. |\n",
    "| **Average Glucose Level** | The distribution is **right-skewed**, with many **high outliers above 200 mg/dL**. These outliers likely represent individuals with **diabetes or metabolic disorders**, which are key risk factors for stroke. Instead of removing them, they should be carefully analyzed. A possible approach is **log transformation** to reduce skewness while keeping these valuable medical insights. |\n",
    "| **BMI (Body Mass Index)** | BMI also shows a **right-skewed distribution**, with extreme values above **50**. These likely represent **obese individuals**, which is a known stroke risk factor. The outliers should not be removed but can be **Winsorized (setting extreme values to the 99th percentile)** to prevent them from disproportionately influencing the model. A **log transformation** can also help mitigate the skewness. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3a5a9-8a17-4008-8e0c-b6c110f0a764",
   "metadata": {},
   "source": [
    "# How to Handle Outliers?\n",
    "## Since this is a medical application, outliers may represent clinically significant cases rather than errors. Instead of removing them, the following approaches can be used:\n",
    "### Winsorization ‚Äì Replace extreme values with the 99th percentile to limit their impact while retaining critical data.\n",
    "### Log Transformation ‚Äì Apply log(1 + x) to compress extreme values while preserving meaningful variations.\n",
    "### Binning ‚Äì Convert continuous variables (e.g., BMI) into meaningful categories like \"Underweight,\" \"Normal,\" \"Overweight,\" and \"Obese\" to help with model interpretability.\n",
    "### Feature Engineering ‚Äì Create a high-risk flag for glucose levels above a certain threshold (e.g., 180 mg/dL) and BMI over 40 to capture the medical significance.\n",
    "## In conclusion, outliers should be retained but transformed or engineered to ensure they contribute meaningfully to stroke prediction.\n",
    "### Why Outlier Treatment Not Done?\n",
    "\n",
    "| Feature              | Min   | Max    | Outlier Treatment? | Justification |\n",
    "|----------------------|-------|--------|--------------------|---------------|\n",
    "| **Age**             | 1     | 82     | ‚ùå No              | Age range is medically valid. No extreme values. |\n",
    "| **BMI**             | 10.3  | 97.6   | ‚ùå No              | Real-world BMI can exceed 204+,or as low as 10.6 so keeping extreme values makes sense. |\n",
    "| **Avg Glucose Level** | 56.63 | 271.74 | ‚ùå No              | Glucose values fit within **hypoglycemia & diabetes** ranges. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313720c-2a4b-4372-a00a-39a7a4d99c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_plots(df, numeric_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "100064c8-504f-4ff7-978a-170c179167e6",
   "metadata": {},
   "source": [
    "# Interpretation Of Violin-Plots\n",
    "### **Interpretation of Distribution, Skewness, and Outliers**\n",
    "\n",
    "| Feature | Interpretation (Distribution, Skewness, and Outliers) |\n",
    "|---------|--------------------------------------------------|\n",
    "| **Age** | The age distribution appears **evenly spread** across all age groups, confirming a uniform distribution. There is **no significant skewness**, meaning all age groups are well represented. The density is **slightly higher for middle-aged individuals (40-60 years old)**. No significant outliers are present. Since age is a crucial stroke predictor, keeping it as a continuous variable is recommended. |\n",
    "| **Average Glucose Level** | The distribution is **right-skewed**, indicating that most individuals have glucose levels in the **lower range (70-120 mg/dL)**, but some extreme values reach **above 200 mg/dL**. The presence of a **distinct peak at higher glucose levels** suggests that a subset of individuals may have **diabetes or metabolic disorders**, which are stroke risk factors. Instead of removing outliers, consider **log transformation** or creating a categorical feature indicating high glucose levels. |\n",
    "| **BMI (Body Mass Index)** | The distribution shows **right skewness**, with most individuals having BMI between **20-35**. However, some extreme outliers **above 50** are visible, representing **severely obese individuals**. Since obesity is a major risk factor for stroke, these values should not be removed. Instead, **Winsorization** (capping extreme values) or **binning BMI into categories** (e.g., underweight, normal, overweight, obese) could help in feature engineering. |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665fa31-f484-4161-98c7-40fa64debdbe",
   "metadata": {},
   "source": [
    "# **Check Normality assumption**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ca962-cdaf-49d9-a89d-b7b235d21321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns\n",
    "numeric_cols = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "# Function to perform Kolmogorov-Smirnov test for normality\n",
    "def ks_test_for_normality(df, numeric_cols):\n",
    "    results = []\n",
    "    for col in numeric_cols:\n",
    "        # Drop NaN values (important for 'bmi' column)\n",
    "        data = df[col].dropna()\n",
    "\n",
    "        # Standardize the data (mean=0, std=1) for KS test\n",
    "        standardized_data = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "        # Perform KS test against standard normal distribution\n",
    "        ks_stat, p_value = stats.kstest(standardized_data, 'norm')\n",
    "\n",
    "        # Interpretation\n",
    "        normality = \"Reject H0 (Not Normal)\" if p_value < 0.05 else \"Fail to Reject H0 (Normal)\"\n",
    "\n",
    "        # Store results\n",
    "        results.append([col, ks_stat, p_value, normality])\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    import pandas as pd\n",
    "    ks_results_df = pd.DataFrame(results, columns=['Feature', 'KS Statistic', 'P-Value', 'Normality'])\n",
    "    \n",
    "    return ks_results_df\n",
    "\n",
    "# Run KS test\n",
    "ks_dataframe = ks_test_for_normality(df, numeric_cols)\n",
    "print(ks_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2008c-fa11-4854-a08c-79321562d040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## So, we plan to use **LOGIT, as base model, and RF and/or XGB as main model** as for them normality is not essential. \n",
    "## If we need to use other algorithm which require assumption of normality then we will come back here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1200ee-fdc3-469b-822c-6b7a8b735fbd",
   "metadata": {},
   "source": [
    "#  **Box-Tidwell Test for Linearity in Log-Odds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac0099-2df4-4408-8948-0841fc1b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df and remove NaN values (important for 'bmi')\n",
    "df_clean = df[['age', 'avg_glucose_level', 'bmi', 'stroke']].dropna()\n",
    "\n",
    "# Add log transformations of numerical variables for interaction term\n",
    "df_clean['age_log'] = df_clean['age'] * np.log(df_clean['age'])\n",
    "df_clean['avg_glucose_level_log'] = df_clean['avg_glucose_level'] * np.log(df_clean['avg_glucose_level'])\n",
    "df_clean['bmi_log'] = df_clean['bmi'] * np.log(df_clean['bmi'])\n",
    "\n",
    "# Define independent variables (without log transformations) and dependent variable\n",
    "X = df_clean[['age', 'avg_glucose_level', 'bmi', 'age_log', 'avg_glucose_level_log', 'bmi_log']]\n",
    "y = df_clean['stroke']\n",
    "\n",
    "# Add intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Display results\n",
    "print(logit_model.summary2().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae4de4-a400-4c64-8abf-f5720bb9d8ab",
   "metadata": {},
   "source": [
    "# **Interpretation of Box-Tidwell Test for Linearity in Log-Odds**\n",
    "\n",
    "### We focus on the **p-values** of the log-transformed interaction terms (`age_log`, `avg_glucose_level_log`, `bmi_log`):\n",
    "\n",
    "| Feature                  | p-value |\n",
    "|--------------------------|---------|\n",
    "| `age_log`               | **0.1549**  |\n",
    "| `avg_glucose_level_log` | **0.7909**  |\n",
    "| `bmi_log`               | **0.5738**  |\n",
    "\n",
    "## **Decision Rule:**\n",
    "## - If **p-value < 0.05**, then the relationship between that variable and log-odds is **not linear** (violation of assumption).\n",
    "## - If **p-value ‚â• 0.05**, then **linearity holds**.\n",
    "\n",
    "## **Conclusion:**\n",
    "###  **All p-values are greater than 0.05**, meaning we **fail to reject the null hypothesis** that the relationship is linear.  \n",
    "### This suggests that **age, avg_glucose_level, and bmi satisfy the linearity assumption**, and **no transformation is needed** for Logistic Regression.  \n",
    "## We can **proceed without log transformation**  (if needed we will do it just to improve model performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28183a1e-70ab-457a-b708-8a0cef6bad5a",
   "metadata": {},
   "source": [
    "# **check for multicollinearity (VIF test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8053c-9de6-4848-9c8a-ee4f6a9e4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent variables (exclude interaction terms used in Box-Tidwell test)\n",
    "X = df[['age', 'avg_glucose_level', 'bmi']].dropna()\n",
    "\n",
    "# Add intercept term (important for VIF calculation)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Display results\n",
    "print(f\"VIF Test Results \\n{vif_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a372d2a-6035-49f3-b1b9-a8b92915bf06",
   "metadata": {},
   "source": [
    "## **Interpretation:**\n",
    "### - **VIF < 5** ‚Üí No multicollinearity concern \n",
    "### - **VIF between 5-10** ‚Üí Moderate multicollinearity (Consider feature selection)  \n",
    "### - **VIF > 10** ‚Üí Severe multicollinearity  (Feature needs to be removed or transformed)  \n",
    "\n",
    "## **Conclusion:**\n",
    "### **All features (`age`, `avg_glucose_level`, `bmi`) have VIF < 5, indicating no multicollinearity issues.**  \n",
    "### The high VIF for `const` is **expected** since it represents the intercept and does not affect the model‚Äôs predictive power.  \n",
    "### We can proceed with **Logistic Regression without multicollinearity concerns.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340a881-c8d9-44df-92fa-1d3dfab5f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_encode(df):\n",
    "    # Step 1: Drop 'id' column\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "    # Step 2: Remove the single row where gender is 'Other'\n",
    "    df = df[df['gender'] != 'Other'].copy()\n",
    "\n",
    "    # Step 4: Impute missing values\n",
    "    df['bmi'].fillna(df['bmi'].median(), inplace=True)  # Median imputation for BMI\n",
    "   \n",
    "    # Encode categorical variables using Label Encoding\n",
    "    cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le  # Store encoders for later use\n",
    "\n",
    "    # Save the encoders to a file\n",
    "    with open('label_encoders.pkl', 'wb') as file:\n",
    "        pickle.dump(label_encoders, file)\n",
    "\n",
    "    # Step 6: Print class distribution of target variable\n",
    "    print(\"Class Distribution of 'stroke':\")\n",
    "    print(df['stroke'].value_counts(normalize=True) * 100)\n",
    "\n",
    "    return df  # Return the preprocessed and encoded DataFrame\n",
    "\n",
    "# Example usage\n",
    "df_processed = preprocess_encode(df)\n",
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c7094-f08d-4580-b93f-9b65fcebc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoders from a file\n",
    "with open('label_encoders.pkl', 'rb') as file:\n",
    "    label_encoders = pickle.load(file)\n",
    "\n",
    "# Dictionary to store mappings\n",
    "encodings = {}\n",
    "\n",
    "# Categorical columns list\n",
    "cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "# Iterate over each categorical column to retrieve and print their mappings\n",
    "for col in cat_cols:\n",
    "    encoder = label_encoders[col]\n",
    "    classes = encoder.classes_\n",
    "    mapping = {label: index for index, label in enumerate(classes)}\n",
    "    encodings[col] = mapping\n",
    "    print(f\"Encoding for {col}:\", mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abfbe4-35c1-4786-a735-708d4cf4be63",
   "metadata": {},
   "source": [
    "# **Bi-Variate And Multi-Variate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f04e2f-7334-4add-96a5-624962bb750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_plot_correlations(df, target_column):\n",
    "    \"\"\"\n",
    "    Computes correlation matrices (Pearson, Spearman, Kendall) and Mutual Information scores.\n",
    "    Plots heatmaps to visualize correlations.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset with numerical features.\n",
    "    target_column (str): The target variable (e.g., 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays correlation heatmaps and MI scores).\n",
    "    \"\"\"\n",
    "    # Drop non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Compute correlation matrices\n",
    "    pearson_corr = df_numeric.corr(method='pearson')\n",
    "    spearman_corr = df_numeric.corr(method='spearman')\n",
    "    kendall_corr = df_numeric.corr(method='kendall')\n",
    "\n",
    "    # Compute Mutual Information Scores\n",
    "    X = df_numeric.drop(columns=[target_column])  # Features\n",
    "    y = df_numeric[target_column]  # Target variable\n",
    "    mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "    mi_scores_df = pd.DataFrame(mi_scores, index=X.columns, columns=['Mutual Info'])\n",
    "\n",
    "    # Plot heatmaps for correlation matrices\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(28,8))\n",
    "\n",
    "    sns.heatmap(pearson_corr, cmap=\"coolwarm\", annot=True, ax=axes[0])\n",
    "    axes[0].set_title(\"Pearson Correlation (Linear)\")\n",
    "\n",
    "    sns.heatmap(spearman_corr, cmap=\"coolwarm\", annot=True, ax=axes[1])\n",
    "    axes[1].set_title(\"Spearman Correlation (Monotonic)\")\n",
    "\n",
    "    sns.heatmap(kendall_corr, cmap=\"coolwarm\", annot=True, ax=axes[2])\n",
    "    axes[2].set_title(\"Kendall Correlation (Ordinal)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Display Mutual Information Scores\n",
    "    print(\"\\nMutual Information Scores (Non-Linear Relationships):\")\n",
    "    print(mi_scores_df.sort_values(by=\"Mutual Info\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9278a-3c46-40d0-8cc4-1867eb2e6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_plot_correlations(df_processed , target_column='stroke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2026756-82fc-443b-a095-4604f7d8db44",
   "metadata": {},
   "source": [
    "# **Interpretation of Pearson Correlation Matrix**\n",
    "### **1. Strongest Correlations (‚â• 0.5)**\n",
    "#### - `ever_married` and `age` (**0.679**) ü°Ü Older individuals are more likely to be married.\n",
    "#### - `work_type` and `ever_married` (**-0.353**) ü°Ü Married individuals tend to have different work distributions than unmarried ones.\n",
    "\n",
    "### **2. Moderate Correlations (0.2 - 0.5)**\n",
    "#### - `age` and `stroke` (**0.245**) ü°Ü Stroke occurrence increases with age.\n",
    "#### - `age` and `bmi` (**0.324**) ü°Ü Older individuals tend to have a higher BMI.\n",
    "#### - `age` and `hypertension` (**0.276**) ü°Ü Older individuals are more likely to have hypertension.\n",
    "#### - `age` and `heart_disease` (**0.264**) ü°Ü Older individuals are more likely to have heart disease.\n",
    "#### - `age` and `avg_glucose_level` (**0.238**) ü°Ü Older individuals tend to have higher glucose levels.\n",
    "#### - `bmi` and `ever_married` (**0.335**) ü°Ü Married individuals tend to have a higher BMI.\n",
    "#### - `smoking_status` and `age` (**0.265**) ü°Ü Older individuals have more defined smoking history.\n",
    "#### - `work_type` and `smoking_status` (**-0.306**) ü°Ü Work type has some relationship with smoking habits.\n",
    "#### - `work_type` and `bmi` (**-0.299**) ü°Ü Different work types may be associated with different BMI levels.\n",
    "\n",
    "### **3. Weak but Relevant Correlations (0.1 - 0.2)**\n",
    "#### - `hypertension` and `stroke` (**0.128**) ü°Ü Hypertension is slightly associated with stroke.\n",
    "#### - `heart_disease` and `stroke` (**0.135**) ü°Ü Heart disease is slightly associated with stroke.\n",
    "####- `avg_glucose_level` and `stroke` (**0.132**) ü°Ü Higher glucose levels are slightly associated with stroke.\n",
    "\n",
    "### **4. Very Weak or No Meaningful Correlation (‚âà 0)**\n",
    "#### - `gender` and `stroke` (**0.009**) ü°Ü No significant relationship.\n",
    "#### - `Residence_type` and `stroke` (**0.015**) ü°Ü No significant relationship.\n",
    "\n",
    "## **Conclusions:**\n",
    "#### - **Age** is a crucial factor affecting multiple variables, including stroke, BMI, hypertension, heart disease, and glucose levels.\n",
    "#### - **Hypertension, heart disease, and glucose levels** have weak but noticeable correlations with stroke.\n",
    "#### - **Marriage status and work type** impact other health indicators but have little effect on stroke.\n",
    "#### - **Gender and residence type** have almost no impact on stroke occurrence.\n",
    "\n",
    "### **Next Steps:**\n",
    "#### - **Check for multicollinearity** (Variance Inflation Factor - VIF test).\n",
    "#### - **Perform logistic regression analysis** to confirm relationships between key predictors and stroke.\n",
    "#### - **Use feature selection techniques** to determine the most predictive variables for stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b5ffd-458e-4dce-a158-ea78e22d88d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Interpretation of Spearman Correlation Matrix**\n",
    "\n",
    "## **Key Observations:**\n",
    "### Spearman correlation measures **monotonic relationships** (whether an increase in one variable results in a consistent increase or decrease in another). It is more robust for capturing **non-linear relationships** than Pearson correlation.\n",
    "\n",
    "### **1. Strongest Correlations (‚â• 0.5)**\n",
    "#### - **`ever_married` and `age`** (**0.664**) ü°Ü Older individuals are more likely to be married.\n",
    "#### - **`ever_married` and `bmi`** (**0.376**) ü°Ü Married individuals tend to have a higher BMI.\n",
    "#### - **`work_type` and `ever_married`** (**-0.324**) ü°Ü Different work types are associated with marital status.\n",
    "\n",
    "### **2. Moderate Correlations (0.2 - 0.5)**\n",
    "#### - **`age` and `stroke`** (**0.250**) ü°Ü Higher age is associated with a higher likelihood of stroke.\n",
    "#### - **`age` and `bmi`** (**0.363**) ü°Ü Older individuals tend to have a higher BMI.\n",
    "#### - **`age` and `hypertension`** (**0.281**) ü°Ü Hypertension becomes more common with increasing age.\n",
    "#### - **`age` and `heart_disease`** (**0.270**) ü°Ü Older individuals are more likely to have heart disease.\n",
    "#### - **`age` and `smoking_status`** (**0.233**) ü°Ü Older individuals have more defined smoking history.\n",
    "#### - **`bmi` and `smoking_status`** (**0.232**) ü°Ü BMI is slightly correlated with smoking history.\n",
    "#### - **`smoking_status` and `ever_married`** (**0.254**) ü°Ü Married individuals tend to have a stronger smoking history.\n",
    "\n",
    "### **3. Weak but Relevant Correlations (0.1 - 0.2)**\n",
    "#### - **`hypertension` and `stroke`** (**0.128**) ü°Ü Hypertension is slightly associated with stroke.\n",
    "#### - **`heart_disease` and `stroke`** (**0.135**) ü°Ü Heart disease has a weak positive correlation with stroke.\n",
    "#### - **`avg_glucose_level` and `stroke`** (**0.083**) ü°Ü Higher glucose levels are weakly associated with stroke.\n",
    "#### - **`bmi` and `stroke`** (**0.051**) ü°Ü Slight positive correlation between BMI and stroke.\n",
    "\n",
    "### **4. Very Weak or No Meaningful Correlation (‚âà 0)**\n",
    "#### - **`gender` and `stroke`** (**0.009**) ü°Ü No significant relationship.\n",
    "#### - **`Residence_type` and `stroke`** (**0.015**) ü°Ü No significant relationship.\n",
    "\n",
    "## **Comparing Spearman vs. Pearson Correlations**\n",
    "#### - The relationships are similar to Pearson correlation but slightly adjusted due to non-linearity.\n",
    "#### - The **`age` and `bmi`** correlation is **stronger in Spearman** (**0.363**) than in Pearson (**0.324**), suggesting a non-linear relationship.\n",
    "#### - The **`age` and `stroke`** correlation is also slightly higher (**0.250**) compared to Pearson (**0.245**), reinforcing that older individuals are more prone to strokes.\n",
    "#### - The **`avg_glucose_level` and `stroke`** correlation is **weaker in Spearman** (**0.083**) than in Pearson (**0.132**), indicating that glucose levels may not have a strong monotonic effect on stroke risk.\n",
    "\n",
    "## **Conclusions:**\n",
    "#### - **Age is the most influential variable**, impacting stroke, hypertension, heart disease, BMI, and marital status.\n",
    "#### - **Hypertension, heart disease, and glucose levels** are weakly associated with stroke.\n",
    "#### - **Marital status and work type** affect BMI, smoking history, and other health indicators.\n",
    "#### - **Gender and residence type have negligible correlations with stroke.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf5ba5-e838-4510-afb7-c449b2aa31a4",
   "metadata": {},
   "source": [
    "# **Interpretation of Kendall Correlation Matrix**\n",
    "\n",
    "### **Key Observations:**\n",
    "### Kendall‚Äôs correlation measures the **ordinal association** between variables. It is useful for datasets with **ranked relationships** and is more robust to outliers than Pearson and Spearman correlations.\n",
    "\n",
    "### **1. Strongest Correlations (‚â• 0.5)**\n",
    "#### **`ever_married` and `age`** (**0.546**) ü°Ü Older individuals are more likely to be married.\n",
    "\n",
    "### **2. Moderate Correlations (0.2 - 0.5)**\n",
    "#### - **`bmi` and `ever_married`** (**0.308**) ü°Ü Married individuals tend to have a higher BMI.\n",
    "#### - **`work_type` and `ever_married`** (**-0.303**) ü°Ü Work type varies significantly based on marital status.\n",
    "#### - **`age` and `bmi`** (**0.246**) ü°Ü BMI tends to increase with age.\n",
    "#### - **`age` and `hypertension`** (**0.231**) ü°Ü Hypertension is more common in older individuals.\n",
    "#### - **`age` and `heart_disease`** (**0.222**) ü°Ü Older individuals are more likely to have heart disease.\n",
    "#### - **`age` and `stroke`** (**0.205**) ü°Ü Stroke occurrence increases with age.\n",
    "#### - **`smoking_status` and `bmi`** (**0.174**) ü°Ü Smoking history is associated with BMI variations.\n",
    "#### - **`ever_married` and `smoking_status`** (**0.234**) ü°Ü Married individuals have distinct smoking patterns.\n",
    "\n",
    "### **3. Weak but Relevant Correlations (0.1 - 0.2)**\n",
    "#### - **`hypertension` and `stroke`** (**0.128**) ü°Ü Hypertension has a weak association with stroke.\n",
    "#### - **`heart_disease` and `stroke`** (**0.135**) ü°Ü Heart disease is weakly correlated with stroke.\n",
    "#### - **`avg_glucose_level` and `stroke`** (**0.067**) ü°Ü Slight positive correlation, but weaker than in Pearson/Spearman.\n",
    "#### - **`bmi` and `stroke`** (**0.042**) ü°Ü Very weak positive correlation.\n",
    "#### - **`work_type` and `bmi`** (**-0.241**) ü°Ü Work type has an impact on BMI.\n",
    "#### - **`work_type` and `smoking_status`** (**-0.262**) ü°Ü Work type is weakly correlated with smoking behavior.\n",
    "\n",
    "\n",
    "### **4. Very Weak or No Meaningful Correlation (‚âà 0)**\n",
    "#### - **`gender` and `stroke`** (**0.009**) ü°Ü No significant relationship.\n",
    "#### - **`Residence_type` and `stroke`** (**0.015**) ü°Ü No significant relationship.\n",
    "\n",
    "\n",
    "## **Comparison: Kendall vs. Spearman vs. Pearson**\n",
    "#### - **Kendall correlations are generally lower** because they measure rank-based relationships rather than absolute values.\n",
    "#### - The **`age` and `stroke`** correlation is **weaker (0.205)** than in Pearson (**0.245**) or Spearman (**0.250**), meaning age may have **a non-monotonic relationship** with stroke risk.\n",
    "#### - The **`avg_glucose_level` and `stroke`** correlation is **significantly weaker (0.067)** in Kendall than in Pearson (**0.132**), suggesting that glucose levels may not follow a strict ranking pattern for stroke prediction.\n",
    "#### - **Marital status (`ever_married`) remains a strong predictor of age** across all three methods.\n",
    "\n",
    "\n",
    "## **Conclusions:**\n",
    "#### - **Age remains the most influential feature**, affecting stroke, BMI, hypertension, and heart disease.\n",
    "#### - **Hypertension, heart disease, and glucose levels** are weakly associated with stroke.\n",
    "#### - **Gender and residence type remain unimportant for stroke prediction.**\n",
    "#### - **Kendall‚Äôs correlation suggests non-linear relationships**, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb3f55-6828-4446-b5d7-5e762fe82018",
   "metadata": {},
   "source": [
    "# **Interpretation of Mutual Information (MI) Scores**\n",
    "\n",
    "### **What is Mutual Information (MI)?**\n",
    "#### - ** Mutual Information measures **non-linear relationships** between features and the target variable (`stroke`). Unlike Pearson, Spearman, and Kendall correlations, MI captures **how much knowing one variable reduces uncertainty about another**, making it useful for detecting complex dependencies.\n",
    "\n",
    "### **Key Observations:**\n",
    "### **1. Features with the Highest Mutual Information (Most Predictive)**\n",
    "#### - **- **`age` (0.035)** ü°Ü The most informative variable for predicting stroke, reinforcing previous correlation findings.\n",
    "#### - **- **`bmi` (0.0097)** ü°Ü Somewhat informative, but weaker than `age`, meaning BMI might have a more complex relationship with stroke risk.\n",
    "#### - **- **`ever_married` (0.0089)** ü°Ü Has some relationship with stroke, likely due to its correlation with age.\n",
    "#### - **- **`heart_disease` (0.0084)** ü°Ü Slightly informative, but weaker than expected given previous correlations.\n",
    "#### - **- **`work_type` (0.0084)** ü°Ü Suggests occupation type has some impact on stroke likelihood.\n",
    "#### - **- **`hypertension` (0.0082)** ü°Ü Has a minor contribution, aligning with weak correlation values.\n",
    "\n",
    "### **2. Features with Low Mutual Information (Less Predictive)**\n",
    "#### - **- **`avg_glucose_level` (0.0064)** ü°Ü Lower than expected, suggesting its relationship with stroke is weak or indirect.\n",
    "#### - **- **`smoking_status` (0.0009)** ü°Ü Almost no contribution to stroke prediction, contradicting some weak correlations observed earlier.\n",
    "#### - **- **`gender` (0.00076)** ü°Ü Insignificant predictor, aligning with previous correlation results.\n",
    "#### - **- **`Residence_type` (0.00013)** ü°Ü The least informative feature, confirming that living in a rural or urban area has **no impact on stroke prediction**.\n",
    "\n",
    "## **Comparison with Correlation Analysis**\n",
    "#### - **`age` is the strongest predictor in both MI and correlation tests.**\n",
    "#### - **`bmi`, `heart_disease`, and `hypertension` have weak but nonzero MI scores**, meaning their impact on stroke is small but not entirely linear.\n",
    "#### - **`avg_glucose_level`, `smoking_status`, `gender`, and `Residence_type` have very low MI scores**, indicating that they are poor predictors of stroke.\n",
    "\n",
    "## **Conclusions:**\n",
    "#### - **Only `age` stands out as a strong predictor.**\n",
    "#### - **Other variables (`bmi`, `heart_disease`, `hypertension`, `ever_married`, `work_type`) have weak relationships with stroke.**\n",
    "#### - **Gender and residence type are practically irrelevant for stroke prediction.**\n",
    "#### - **Non-linear methods (e.g., decision trees, gradient boosting) might better capture interactions between variables.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0d60c-0b74-43c3-8314-bdaf693ce380",
   "metadata": {},
   "source": [
    "# **Box Plots (Numerical Features vs. Stroke)**\n",
    "## Box plots show the distribution of numerical features across stroke (0 vs. 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07839c50-2a21-4e03-ac9f-89a5aec4db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates box plots for numerical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the box plots).\n",
    "    \"\"\"\n",
    "    num_features = ['age', 'avg_glucose_level', 'bmi']  # Numeric columns\n",
    "    for col in num_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(x=df[target_column], y=df[col])\n",
    "        plt.title(f\"Box Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(target_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c1de7-f5cd-48d1-81ad-2bebc6fba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df_processed)  # Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9057455-c47b-43c0-82f6-ddced1946104",
   "metadata": {},
   "source": [
    "# **Interpretation of Box Plots (Bivariate Analysis with Stroke)**\n",
    "\n",
    "| **Feature**             | **Observations** |\n",
    "|------------------------|----------------|\n",
    "| **Age**               | Individuals who experienced a stroke tend to be significantly older compared to those who did not. The median age of stroke patients is much higher. Very few young individuals have had a stroke. |\n",
    "| **Avg Glucose Level**  | Stroke patients have a much wider range of glucose levels, with higher median values. Many outliers exist among non-stroke individuals, suggesting high glucose levels even in those without a stroke. |\n",
    "| **BMI**               | BMI distributions for stroke and non-stroke individuals are similar, with slight differences. There are some high-BMI outliers in both groups, but BMI does not show a strong separation between stroke and non-stroke cases. |\n",
    "\n",
    "**Key Insights:**\n",
    "- **Age is a strong differentiator** between stroke and non-stroke individuals.\n",
    "- **Avg Glucose Level is notably higher in stroke patients**, suggesting a possible relationship with diabetes or metabolic conditions.\n",
    "- **BMI does not show a significant distinction** between the two groups, though extreme BMI values exist in both categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a3de1-9981-4ae5-9e5f-46324d4130a6",
   "metadata": {},
   "source": [
    "# **Performing statistical tests (e.g., t-tests) to confirm the significance of these differences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6bcecf-e3f0-45ef-906e-d51b6df2f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "stroke_0 = df_processed[df_processed['stroke'] == 0]\n",
    "stroke_1 = df_processed[df_processed['stroke'] == 1]\n",
    "\n",
    "# Performing independent t-tests and Mann-Whitney U tests\n",
    "results = []\n",
    "for feature in numerical_features:\n",
    "    t_stat, t_pval = stats.ttest_ind(stroke_0[feature], stroke_1[feature], equal_var=False)\n",
    "    u_stat, u_pval = stats.mannwhitneyu(stroke_0[feature], stroke_1[feature], alternative='two-sided')\n",
    "    results.append([feature, t_stat, t_pval, u_stat, u_pval])\n",
    "# Creating a results DataFrame\n",
    "stats_results_df = pd.DataFrame(results, columns=['Feature', 'T-Test Statistic', 'T-Test p-value', 'Mann-Whitney U Statistic', 'Mann-Whitney p-value'])\n",
    "# Display results\n",
    "print(stats_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6e3fc-6ac6-4c1c-b7f8-8de391e6be40",
   "metadata": {},
   "source": [
    "# **Interpretation of Statistical Tests (T-Test and Mann-Whitney U Test)**\n",
    "\n",
    "| **Feature**           | **T-Test Statistic** | **T-Test p-value** | **Mann-Whitney U Statistic** | **Mann-Whitney p-value** | **Observations** |\n",
    "|----------------------|--------------------|------------------|--------------------------|----------------------|----------------|\n",
    "| **Age**             | -29.68              | 2.18e-95         | 200261.5                 | 3.85e-71             | Strong evidence that stroke patients are significantly older than non-stroke patients. |\n",
    "| **Avg Glucose Level** | -6.98               | 2.37e-11         | 471082.0                 | 3.58e-09             | Stroke patients have significantly higher glucose levels, but the effect size is smaller compared to age. |\n",
    "| **BMI**             | -3.33               | 9.94e-04         | 522627.0                 | 2.81e-04             | There is a statistically significant difference in BMI, but the effect size is weak. |\n",
    "\n",
    "**Key Insights:**\n",
    "- **Age is the most statistically significant feature**, with an extremely low p-value in both tests, confirming its strong association with stroke.\n",
    "- **Avg Glucose Level is also significant**, indicating that higher glucose levels are correlated with stroke.\n",
    "- **BMI shows statistical significance but with a weaker effect**, suggesting it may not be a strong predictor of stroke on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e88490-5f4a-4bce-9348-783aae1b497e",
   "metadata": {},
   "source": [
    "# **Bar Plots (Categorical Features vs. Stroke)**\n",
    "## Bar plots show the distribution of stroke occurrence across categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f14c79-6b36-4e26-a301-7e152b336a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barplots(df, target_column='stroke'):\n",
    "    \"\"\" \n",
    "    Creates bar plots for categorical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with categorical features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the bar plots).\n",
    "    \"\"\"\n",
    "    target_column = 'stroke'\n",
    "    cat_features = df.select_dtypes(include=['bool', 'int64']).columns.drop(target_column).tolist()\n",
    "    '''\n",
    "    cat_features = ['gender_Male', 'ever_married_Yes', 'Residence_type_Urban', 'work_type_Never_worked', 'work_type_Private', 'work_type_Self-employed',\n",
    "                    'work_type_children', 'smoking_status_formerly smoked', 'smoking_status_Unknown','smoking_status_never smoked', 'smoking_status_smokes']\n",
    "    '''\n",
    "    for col in cat_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=df[col], y=df[target_column])\n",
    "        plt.title(f\"Bar Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(f\"Mean {target_column}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ed73a-2825-48ae-b25c-036b39a45fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplots(df_processed)  # Bar plots for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64969d2c-15df-4a23-b9b8-d2c8e900515b",
   "metadata": {},
   "source": [
    "# **Interpretation of Bar Plots (Categorical Features vs. Stroke)**\n",
    "\n",
    "| **Feature**                     | **Observations** |\n",
    "|--------------------------------|----------------|\n",
    "| **Hypertension** | Individuals with hypertension have a **significantly higher stroke prevalence** compared to those without hypertension. The stroke rate is over three times higher in hypertensive individuals. |\n",
    "| **Heart Disease** | Individuals with heart disease have an **even higher stroke prevalence** compared to those without it. The stroke rate is approximately **four times higher** in people with heart disease. |\n",
    "| **Smoking Status - Unknown**   | Individuals with **unknown smoking status (1)** have a **lower stroke prevalence** compared to those with known smoking status (0). \n",
    "| **Gender (Male vs. Female)**  | No significant difference in stroke prevalence between males and females. |\n",
    "| **Ever Married**               | Stroke prevalence is much higher among individuals who have ever been married, suggesting age-related effects. |\n",
    "| **Residence Type (Urban vs. Rural)** | Stroke prevalence is nearly the same for urban and rural residents, indicating no strong association. |\n",
    "| **Work Type - Never Worked**   | Individuals who never worked have a slightly higher stroke prevalence. However, the sample size may be small. |\n",
    "| **Work Type - Private Sector** | Stroke prevalence is slightly higher for individuals working in the private sector, but the difference is small. |\n",
    "| **Work Type - Self-Employed**  | Self-employed individuals show a higher stroke prevalence compared to other work types. |\n",
    "| **Work Type - Children**       | Children category has very low stroke prevalence, as expected. |\n",
    "| **Smoking Status - Formerly Smoked** | Individuals who formerly smoked show a higher stroke prevalence, supporting smoking as a risk factor. |\n",
    "| **Smoking Status - Never Smoked** | No significant difference in stroke prevalence between those who never smoked and others. |\n",
    "| **Smoking Status - Smokes** | Stroke prevalence is slightly higher for current smokers, but the difference is not as pronounced as in former smokers. |\n",
    "\n",
    "# **Key Insights:**\n",
    "- **Hypertension is a major risk factor for stroke**, as shown by the significantly higher stroke rate in hypertensive individuals.\n",
    "- **Heart disease has an even stronger association with stroke**, suggesting that individuals with cardiovascular conditions are at extremely high risk.\n",
    "- The error bars indicate some variability, but the difference in stroke prevalence is large enough to suggest a real association.\n",
    "- **Marital status and age are closely linked**, which explains the higher stroke prevalence among those who have ever been married.\n",
    "- **Self-employed individuals have a higher stroke prevalence**, which might be due to lifestyle factors or health access disparities.\n",
    "- **Smoking history plays a role**, with former smokers showing the highest stroke prevalence.\n",
    "- **Residence type does not appear to influence stroke risk significantly.**\n",
    "- The lower stroke prevalence among individuals with **unknown smoking status** suggests that missing or unreported smoking data may be associated with a healthier subgroup.\n",
    "- This could indicate **reporting bias**, where healthier individuals are less likely to have smoking history recorded.\n",
    "- Further analysis is needed to determine whether this category should be dropped, imputed, or treated separately in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9a9f2-6448-4cfc-bec4-6ab07d54f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df, target_column):\n",
    "    \"\"\"\n",
    "    Perform the Chi-Square test of independence for categorical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The encoded DataFrame containing categorical features.\n",
    "    target_column (str): The name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the Chi-Square statistic and p-values for each categorical feature.\n",
    "    \"\"\"\n",
    "    cat_features = df.select_dtypes(include=['bool', 'int64']).columns.drop(target_column).tolist()\n",
    "    results = []\n",
    "\n",
    "    for feature in cat_features:\n",
    "        contingency_table = pd.crosstab(df[feature], df[target_column])\n",
    "        chi2_stat, p_val, _, _ = stats.chi2_contingency(contingency_table)\n",
    "        results.append([feature, chi2_stat, p_val])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['Feature', 'Chi-Square Statistic', 'p-value'])\n",
    "    return results_df\n",
    "   \n",
    "results_df = chi_square_test(df_processed, target_column='stroke')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873871d-8d22-498a-9487-ba263a391c6a",
   "metadata": {},
   "source": [
    "# **Interpretation of Chi-Square Test for Categorical Features vs. Stroke**\n",
    "\n",
    "## The **Chi-Square test** evaluates whether there is a **statistically significant association** between categorical features and the target variable (`stroke`).\n",
    "\n",
    "### **Chi-Square Test Results:**\n",
    "| Feature           | Chi-Square Statistic | p-value |\n",
    "|------------------|---------------------|----------------|\n",
    "| `gender`        | 0.34                 | **0.5598** (Not significant) |\n",
    "| `hypertension`  | 81.57                | **1.69e-19** (Significant) |\n",
    "| `heart_disease` | 90.23                | **2.12e-21** (Significant) |\n",
    "| `ever_married`  | 58.87                | **1.69e-14** (Significant) |\n",
    "| `work_type`     | 49.16                | **5.41e-10** (Significant) |\n",
    "| `Residence_type` | 1.07                | **0.2998** (Not significant) |\n",
    "| `smoking_status` | 29.23                | **2.01e-06** (Significant) |\n",
    "\n",
    "## **Key Insights:**\n",
    "### **Hypertension, heart disease, ever_married, work_type, and smoking_status** show strong associations with `stroke` (p-value < 0.05), meaning they are likely relevant predictors.  \n",
    "### **Gender and Residence_type are not significantly associated with stroke**, confirming earlier results from mutual information scores and correlation analysis.  \n",
    "### These results align with **bar plot insights**, where `hypertension` and `heart_disease` showed the highest stroke prevalence.\n",
    "### Since we have already analyzed **mutual information, correlation, and bar plots**, the Chi-Square test **reconfirms** our findings.\n",
    "### It is **not redundant** because it **quantifies** statistical significance rather than just showing trends visually.\n",
    "### **Useful takeaway**: We can **exclude `gender` and `Residence_type`** from modeling due to their **weak statistical association**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330bfc1-8b5f-4467-885d-bfb5ae98f531",
   "metadata": {},
   "source": [
    "# **Violin Plots (For Distribution Comparison)**\n",
    "## Violin plots show the spread of numerical features by stroke category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4f8a0-e4e8-42ab-a3e5-660b4d5512a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violinplots(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates violin plots for numerical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the violin plots).\n",
    "    \"\"\"\n",
    "    num_features = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    for col in num_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.violinplot(x=df[target_column], y=df[col])\n",
    "        plt.title(f\"Violin Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(target_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445af77d-10da-450b-ada8-95d5cc5ed1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violinplots(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ce9d4-551d-45bb-b484-b53430b15ae9",
   "metadata": {},
   "source": [
    "# **Violin Plot Interpretation: Numerical Features vs. Stroke**\n",
    "| **Feature**             | **Observations** |\n",
    "|------------------------|----------------|\n",
    "| **Age**               | The distribution of age is significantly different between stroke and non-stroke groups. Stroke patients tend to be older, with a higher density of cases above **60 years**. The non-stroke group has a wider spread across all ages. |\n",
    "| **Avg Glucose Level**  | Stroke patients tend to have a higher average glucose level distribution, with more cases having **glucose levels above 150**. The non-stroke group has a **lower concentration** at extreme glucose levels but still contains some outliers. |\n",
    "| **BMI**               | The BMI distribution is **similar** between stroke and non-stroke groups, with no strong separation. There are some high-BMI outliers in both groups, but the overall shape is quite similar. |\n",
    "\n",
    "## **Key Insights:**\n",
    "- **Age is the most differentiating feature**, with stroke cases **concentrated in older individuals**.\n",
    "- **Glucose levels show some separation**, with stroke patients having a **higher proportion of extreme values**.\n",
    "- **BMI does not show a strong difference**, meaning it may not be a major factor in distinguishing stroke vs. non-stroke cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728f0e8-a4bf-4641-af00-7f6b4fd18c8d",
   "metadata": {},
   "source": [
    "# **Testing above visual findings by statistical tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3b4b5-2338-4834-82eb-75dd46b38e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_tests(df):\n",
    "    \"\"\"\n",
    "    Performs Kolmogorov-Smirnov (KS) and Mann-Whitney U tests to compare numerical feature distributions\n",
    "    between two classes in the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataset.\n",
    "    target_column (str): The binary target variable (default is 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the KS test and Mann-Whitney U test results.\n",
    "    \"\"\"\n",
    "    numerical_features = df.select_dtypes(include=['float64'])\n",
    "    target_column = 'stroke'\n",
    "    results = []\n",
    "    for feature in numerical_features:\n",
    "        group_0 = df[df[target_column] == 0][feature].dropna()\n",
    "        group_1 = df[df[target_column] == 1][feature].dropna()\n",
    "        \n",
    "        # Kolmogorov-Smirnov Test\n",
    "        ks_stat, ks_pvalue = ks_2samp(group_0, group_1)\n",
    "        \n",
    "        # Mann-Whitney U Test\n",
    "        mw_stat, mw_pvalue = mannwhitneyu(group_0, group_1, alternative='two-sided')\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature,\n",
    "            'KS Statistic': ks_stat,\n",
    "            'KS p-value': ks_pvalue,\n",
    "            'Mann-Whitney U Statistic': mw_stat,\n",
    "            'Mann-Whitney p-value': mw_pvalue\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f806187-9434-4776-a236-20dd4b88a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_mw_results = perform_statistical_tests(df_processed)\n",
    "print(ks_mw_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a36ab-dd27-4b4f-97da-59b4a6e50081",
   "metadata": {},
   "source": [
    "# **Interpretation of Kolmogorov-Smirnov & Mann-Whitney U Tests**\n",
    "\n",
    "| **Feature**             | **KS Statistic** | **KS p-value** | **Mann-Whitney U Statistic** | **Mann-Whitney p-value** | **Observations** |\n",
    "|------------------------|---------------|--------------|--------------------------|----------------------|----------------|\n",
    "| **Age**               | 0.5408        | 3.99e-65     | 200261.5                 | 3.85e-71             | **Highly significant distribution difference** between stroke and non-stroke groups. Age is the most strongly differentiating feature. |\n",
    "| **Avg Glucose Level**  | 0.2401        | 1.67e-12     | 471082.0                 | 3.58e-09             | **Significant difference in glucose distribution**, with stroke patients having higher glucose levels. |\n",
    "| **BMI**               | 0.1920        | 4.02e-08     | 522627.0                 | 2.81e-04             | **Statistically significant**, but BMI has the weakest separation among the three numerical features. |\n",
    "\n",
    "## **How Do These Results Match the Violin Plot Analysis?**\n",
    "1. **Age (Strongest Differentiator)**\n",
    "   - The violin plot showed that **stroke patients are predominantly older**, with their distribution heavily concentrated above **60 years**.\n",
    "   - The **KS test confirms this**, showing a **very high KS statistic (0.54) and extremely low p-value (<1e-60)**.\n",
    "   - The **Mann-Whitney U test reinforces this**, indicating that age distributions are vastly different.\n",
    "\n",
    "2. **Avg Glucose Level (Moderate Differentiator)**\n",
    "   - The violin plot showed that **stroke patients have higher glucose levels**, with a wider spread and more extreme values compared to non-stroke individuals.\n",
    "   - The **KS statistic (0.24) and p-value (<1e-10) confirm this** difference is statistically significant.\n",
    "   - The **Mann-Whitney U test also supports** that glucose levels are meaningfully different between stroke and non-stroke groups.\n",
    "\n",
    "3. **BMI (Weakest Differentiator)**\n",
    "   - The violin plot **showed some overlap** between stroke and non-stroke groups, meaning BMI does not separate the two as clearly as age or glucose levels.\n",
    "   - The **KS statistic (0.19) is the lowest**, suggesting that while BMI distributions are different, the separation is not as strong as in age or glucose.\n",
    "   - The **Mann-Whitney p-value (<1e-3) confirms BMI is still statistically significant**, but it has the weakest effect among the three features.\n",
    "\n",
    "## **Key Takeaways**\n",
    "**Age is the most important factor** with the highest statistical significance and strongest visual separation.  \n",
    "**Glucose levels also show a meaningful difference**, with stroke patients having higher glucose values.  \n",
    "**BMI is statistically significant but less impactful**, aligning with the violin plot which showed more overlap between stroke and non-stroke groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684986-2b5a-4c38-9457-5a7054462116",
   "metadata": {},
   "source": [
    "# KDE Plots (For Probability Density Comparison)\n",
    "## KDE (Kernel Density Estimation) plots help compare feature distributions for stroke vs. non-stroke cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca0198-9921-433c-8baf-805758be93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kdeplots(df):\n",
    "    \"\"\"\n",
    "    Creates KDE plots for numerical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the KDE plots).\n",
    "    \"\"\"\n",
    "    num_features = df.select_dtypes(include=['float64'])\n",
    "    target_column = 'stroke'\n",
    "    for col in num_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.kdeplot(data=df[df[target_column] == 0], x=col, label=\"No Stroke\", fill=True)\n",
    "        sns.kdeplot(data=df[df[target_column] == 1], x=col, label=\"Stroke\", fill=True)\n",
    "        plt.title(f\"KDE Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a754-6372-44a6-b35c-143d3952dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kdeplots(df_processed)  # KDE plots for density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d47a5e-b335-472c-af5d-945dbc5cfab8",
   "metadata": {},
   "source": [
    "# **Interpretation of KDE Plots**\n",
    "\n",
    "| **Feature**             | **Observations** |\n",
    "|------------------------|----------------|\n",
    "| **Age**               | Stroke patients tend to be **significantly older**. The non-stroke group has a **wider age distribution**, while the stroke group is **heavily concentrated above 60 years**. This confirms that **age is a major risk factor** for stroke. |\n",
    "| **Avg Glucose Level**  | Stroke patients have a **higher glucose distribution** with **two peaks**, suggesting that stroke is more common in individuals with **extremely high glucose levels**. The non-stroke group has a **narrower, more centered distribution** around normal glucose levels. This aligns with **diabetes being a major risk factor**. |\n",
    "| **BMI**               | The distributions for stroke and non-stroke groups **overlap significantly**, indicating that BMI **does not strongly differentiate between the two groups**. However, stroke patients have **a slightly higher density in higher BMI values**, suggesting that obesity might still be a contributing factor. |\n",
    "\n",
    "## **How These Results Align with Previous Analysis**\n",
    "**Age is the most important differentiator**, with a clear separation in the KDE plot, matching results from the **violin plot, KS test, and Mann-Whitney U test**.  \n",
    "**Glucose levels show moderate separation**, reinforcing its **moderate feature importance** \n",
    "**BMI distributions overlap significantly**, matching its **lower statistical significance** and **weaker predictive power** from previous tests.\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **Age and glucose levels should be key predictors** in the model.\n",
    "- **BMI might have some influence**, but it is less predictive and may not require special transformations.\n",
    "- **For Logistic Regression**, consider **scaling glucose levels** due to its spread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d153d2b-b043-4cac-9db1-6c7ad10680eb",
   "metadata": {},
   "source": [
    "# 1.Strip Plot \n",
    "### Shows individual data points for numerical variables across stroke categories. Helps in identifying data density and clustering patterns.\n",
    "### **Why? Here a swarm plot overlaps too much due to a large dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c62004-2fd4-46c9-bf2b-a96b20073ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stripplots(df):\n",
    "    \"\"\"\n",
    "    Creates strip plots for numerical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the strip plots).\n",
    "    \"\"\"\n",
    "    num_features = df.select_dtypes(include=['float64'])\n",
    "    target_column = 'stroke'\n",
    "    for col in num_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.stripplot(x=df[target_column], y=df[col], jitter=True, alpha=0.5)\n",
    "        plt.title(f\"Strip Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(target_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc4138-83f9-4a23-b2fb-43566b59b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stripplots(df_processed)  # Strip plots for numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad8086-d27b-4426-957a-ceba16fef63f",
   "metadata": {},
   "source": [
    "# **Interpretation of Strip Plots (Numerical Features vs. Stroke)**\n",
    "\n",
    "| **Feature**             | **Observations** |\n",
    "|------------------------|----------------|\n",
    "| **Age**               | Stroke cases are **concentrated in older individuals** (mostly above **60 years**). The non-stroke group has a much **wider age range**, including younger individuals. This confirms that **age is a major risk factor** for stroke. |\n",
    "| **Avg Glucose Level**  | Stroke patients tend to have **higher glucose levels**, with several cases exceeding **150+ glucose level**. The non-stroke group has more **compact clustering around normal glucose levels (~80-100)**. This supports the idea that **elevated glucose levels (possibly diabetes) increase stroke risk**. |\n",
    "| **BMI**               | The distribution of BMI values **overlaps significantly** between stroke and non-stroke groups, suggesting that BMI alone is **not a strong differentiator**.|\n",
    "\n",
    "## **Key Takeaways**\n",
    "**Age shows the strongest visual separation**, aligning with previous findings from violin plots, KDE plots, and statistical tests.  \n",
    "**Glucose levels also exhibit meaningful differences**, reinforcing its moderate predictive power.  \n",
    "**BMI is less effective in separating stroke from non-stroke cases**, as its distribution is widely spread across both groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df4303-5a83-4cae-af5e-635e5025e9a2",
   "metadata": {},
   "source": [
    "# **Count Plot (For Categorical Features)**\n",
    "### Counts the number of observations for each category grouped by stroke.\n",
    "### Why? Helps in understanding imbalance or trends in categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed63760-87a7-4d3e-bf37-df163520a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_countplots(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates count plots for categorical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with categorical features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the count plots).\n",
    "    \"\"\"\n",
    "    cat_features = df.select_dtypes(include=['bool', 'int64']).columns.drop(target_column).tolist()\n",
    "    \n",
    "    for col in cat_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(x=df[col], hue=df[target_column])\n",
    "        plt.title(f\"Count Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend(title=target_column, loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7d845-8172-4bc0-928a-08136bea035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplots(df_processed)  # Count plots for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596b8c1-b8ee-425e-a91e-73ffbaa21ba4",
   "metadata": {},
   "source": [
    "# **Interpretation of Count Plots (Categorical Features vs. Stroke)**\n",
    "\n",
    "## **Feature-wise Observations**\n",
    "| Feature            | Observations |\n",
    "|--------------------|-------------|\n",
    "| **Gender**        | Stroke prevalence is similar between males and females, indicating no strong association between gender and stroke. |\n",
    "| **Hypertension**  | Individuals with hypertension show a higher prevalence of stroke, reinforcing its role as a risk factor. |\n",
    "| **Heart Disease** | Stroke cases are significantly more common among individuals with heart disease, making it a strong predictor. |\n",
    "| **Ever Married**  | Stroke cases are more frequent among individuals who have ever been married, possibly due to age-related effects. |\n",
    "| **Work Type**     | Stroke prevalence is highest among individuals in government jobs and self-employed individuals. The \"never worked\" category has very few stroke cases, likely due to small sample size. |\n",
    "| **Residence Type**| No significant difference in stroke prevalence between urban and rural residents, suggesting that living location does not have a major impact. |\n",
    "| **Smoking Status**| The \"formerly smoked\" category has a higher stroke prevalence, supporting smoking as a potential risk factor. The \"unknown\" category has slightly lower stroke prevalence, likely due to missing or unreported data. |\n",
    "\n",
    "## **Key Insights**\n",
    "###  **Heart disease and hypertension** are strong predictors of stroke, with much higher prevalence among stroke patients.\n",
    "### **Age-related factors** (ever married and certain work types) show some association with stroke, possibly due to older populations being more at risk.\n",
    "### **Smoking status** shows a pattern, where former smokers exhibit a slightly higher stroke prevalence than current smokers.\n",
    "### **Gender and residence type** do not appear to have a strong impact on stroke prevalence.\n",
    "\n",
    "## These findings align with previous statistical tests and visualization results, confirming the importance of medical conditions and lifestyle factors in predicting stroke risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee01e56-9226-42a3-8a1a-8cdde414ef6d",
   "metadata": {},
   "source": [
    "# **Heatmap of Feature Distributions (By Stroke)**\n",
    "## Instead of a correlation matrix, this shows how feature values vary across stroke vs. non-stroke cases.\n",
    "## Why? Helps spot patterns across multiple features simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c672050-7322-4d0a-8d33-7e6aed7c44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution_heatmap(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates a heatmap of feature distributions by stroke.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the heatmap).\n",
    "    \"\"\"\n",
    "    num_features = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    \n",
    "    # Compute mean values for each feature per stroke category\n",
    "    heatmap_data = df.groupby(target_column)[num_features].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(heatmap_data.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Average Feature Values by Stroke Category\")\n",
    "    plt.xlabel(\"Stroke Category\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69f36c-789e-4c5b-b9c4-612b3ce6f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution_heatmap(df_processed)  # Feature heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd82c2-7480-40a0-96f9-cf5258034b51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Interpretation of Feature Distribution Heatmap (By Stroke Category)**\n",
    "\n",
    "| **Feature**            | **Stroke = 0 (No Stroke)** | **Stroke = 1 (Stroke)** | **Observations** |\n",
    "|------------------------|------------------|----------------|----------------|\n",
    "| **Age**               | 41.97            | 67.73          | Stroke patients are **significantly older** on average. This confirms that **age is a major risk factor** for stroke. |\n",
    "| **Avg Glucose Level**  | 104.79           | 132.54         | Stroke patients have a **higher average glucose level**, supporting the link between **high blood sugar (diabetes) and stroke risk**. |\n",
    "| **BMI**               | 28.80            | 30.09          | BMI is **slightly higher** in stroke patients, but the difference is small. This suggests that **obesity may have a minor effect** compared to age and glucose levels. |\n",
    "---\n",
    "### **Key Takeaways**\n",
    "**Age is the strongest differentiator**, aligning with previous statistical tests and visualizations.  \n",
    "**Glucose levels are also significantly higher** in stroke patients, reinforcing its predictive importance.  \n",
    "**BMI shows only a small increase**, suggesting that its impact on stroke might be secondary compared to other factors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614fefd5-b37a-4619-a1dc-517675e04645",
   "metadata": {},
   "source": [
    "# Statistical test to quantify significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc41fee-9ed8-459b-bc23-32d9b09c2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_tests(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Performs T-tests and Mann-Whitney U tests on numerical features to check statistical significance\n",
    "    between stroke (1) and non-stroke (0) groups.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe containing numerical features.\n",
    "    target_column (str): The target variable column name (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame summarizing the test results.\n",
    "    \"\"\"\n",
    "    numerical_features = df.select_dtypes(include=['float64'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        # Splitting data into stroke and non-stroke groups\n",
    "        group_0 = df[df[target_column] == 0][feature]\n",
    "        group_1 = df[df[target_column] == 1][feature]\n",
    "\n",
    "        # Performing T-test\n",
    "        t_stat, t_pval = stats.ttest_ind(group_0, group_1, equal_var=False)\n",
    "\n",
    "        # Performing Mann-Whitney U test\n",
    "        mw_stat, mw_pval = stats.mannwhitneyu(group_0, group_1, alternative='two-sided')\n",
    "\n",
    "        results.append([feature, t_stat, t_pval, mw_stat, mw_pval])\n",
    "\n",
    "    # Converting results to a DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=['Feature', 'T-Test Statistic', 'T-Test p-value', \n",
    "                                                'Mann-Whitney U Statistic', 'Mann-Whitney p-value'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "statistical_test_results = perform_statistical_tests(df_processed, target_column='stroke')\n",
    "\n",
    "# Display results\n",
    "statistical_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f90873-2316-46fa-90af-76940a99de2c",
   "metadata": {},
   "source": [
    "# Interpretation Of Statistical Test results : Stroke vs. Non-Stroke Groups**\n",
    "\n",
    "| **Feature**           | **T-Test Statistic** | **T-Test p-value** | **Mann-Whitney U Statistic** | **Mann-Whitney p-value** | **Observations** |\n",
    "|----------------------|--------------------|-------------------|--------------------------|-----------------------|----------------|\n",
    "| **Age**             | -29.68             | **2.17e-95**      | 200261.5                 | **3.84e-71**          | **Strongly significant** ‚Äì Stroke group is **significantly older** than non-stroke group. |\n",
    "| **Avg Glucose Level** | -6.98              | **2.37e-11**      | 471082.0                 | **3.58e-09**          | **Statistically significant** ‚Äì Stroke patients have **higher glucose levels**, confirming its impact on stroke risk. |\n",
    "| **BMI**             | -3.32              | **9.94e-04**      | 522627.0                 | **2.81e-04**          | **Statistically significant** ‚Äì But weaker than Age and Glucose Level, suggesting BMI's effect is **less prominent**. |\n",
    "\n",
    "---\n",
    "### **Key Takeaways**\n",
    "**All three features (Age, Avg Glucose Level, BMI) show significant differences** between stroke and non-stroke groups.  \n",
    "**Age is the most significant predictor** (extremely low p-value).  \n",
    "**Glucose levels also strongly differentiate stroke vs. non-stroke patients.**  \n",
    "**BMI has a smaller but still significant effect.**  \n",
    "\n",
    "### **How This Aligns with Previous Visualizations**\n",
    "- **Violin Plots & KDE Plots:** Confirm that stroke patients tend to be older and have higher glucose levels.  \n",
    "- **Box Plots:** Showed that the median **age and glucose levels** are higher in stroke cases.  \n",
    "- **Heatmap of Mean Values:** Highlighted the clear differences in **age and glucose levels** for stroke patients.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c68c6-9807-41f3-a1d4-2ba95fcaaf37",
   "metadata": {},
   "source": [
    "# Boxen Plot (Improved Box Plot for Large Data)\n",
    "## Boxen plots are an advanced version of box plots, showing more extreme quantiles.\n",
    "## **Why? Helps better visualize outliers and spread in large datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2161fc9-4a18-4f10-9047-7b8fa701a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxenplots(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates boxen plots for numerical features against the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the boxen plots).\n",
    "    \"\"\"\n",
    "    num_features = df.select_dtypes(include=['float64'])\n",
    "    for col in num_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxenplot(x=df[target_column], y=df[col])\n",
    "        plt.title(f\"Boxen Plot of {col} by {target_column}\")\n",
    "        plt.xlabel(target_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfabc9-bf2a-4172-a13f-072271a728a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxenplots(df_processed)  # Boxen plots for better visualization of spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d56d0-2602-4dc6-9a14-f5faa7009492",
   "metadata": {},
   "source": [
    "# Interpretation of boxen plots\n",
    "| **Feature**            | **Non-Stroke Group (0)**                           | **Stroke Group (1)**                          | **Key Takeaway** |\n",
    "|------------------------|----------------------------------------------------|-----------------------------------------------|------------------|\n",
    "| **Age**               | - Lower median age.<br>- More concentration in younger age groups.<br>- Fewer outliers. | - Significantly higher median age.<br>- Wider interquartile range (IQR).<br>- More variability in older ages. | **Older individuals have a much higher stroke risk.** |\n",
    "| **Avg Glucose Level** | - Lower median glucose levels.<br>- More concentrated around normal values.<br>- Fewer extreme values. | - Higher median glucose levels.<br>- More extreme outliers (above 200).<br>- Wider spread in distribution. | **High glucose levels (potential diabetes) are a strong risk factor for stroke.** |\n",
    "| **BMI**               | - More concentrated around normal BMI ranges.<br>- Fewer extreme values. | - Slightly higher median BMI.<br>- More outliers in higher BMI range.<br>- Similar overall distribution. | **Obesity is a contributing factor but weaker than age & glucose.** |\n",
    "\n",
    "### **Summary**\n",
    "**Age and Avg Glucose Level are the strongest differentiators between stroke and non-stroke groups.**  \n",
    "**BMI shows some effect, but its impact is weaker compared to the other two features.**  \n",
    "**Presence of extreme values in stroke cases suggests possible outlier treatment before modeling. But we earlier decided against it.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e29a2-674f-4370-a507-8e1c661f559d",
   "metadata": {},
   "source": [
    "# Pair Plot (Multivariate Exploration)\n",
    "### A scatterplot matrix that shows relationships between numerical features grouped by stroke.\n",
    "### Why? Helps see patterns in how features interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8aa1e-ab93-465d-addb-3f02cb54d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairplot(df, target_column='stroke'):\n",
    "    \"\"\"\n",
    "    Creates a pairplot of numerical features colored by the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Encoded dataframe with numeric features.\n",
    "    target_column (str): The target variable (default: 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays the pair plot).\n",
    "    \"\"\"\n",
    "    num_features = ['age', 'avg_glucose_level', 'bmi', target_column]\n",
    "    sns.pairplot(df_processed[num_features], hue=target_column, diag_kind=\"kde\", palette=\"husl\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad31c2b-017b-4f4c-8e0c-22ee8ee0feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c954f-f8be-4af2-9249-88f8a4624a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairplot(df_processed)  # Pair plot for numerical relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52003409-3e8e-45b6-a1fb-179709581697",
   "metadata": {},
   "source": [
    "### Interpretation of Pair Plot (Multivariate Exploration)\n",
    "\n",
    "| Feature Combination          | Observation | Interpretation |\n",
    "|------------------------------|-------------|----------------|\n",
    "| **Age vs. Stroke**           | Stroke cases (teal points) are concentrated in **older age groups**. | Older individuals are at higher risk of stroke. |\n",
    "| **Avg Glucose Level vs. Stroke** | Higher glucose levels are **more common** in stroke cases. | High glucose levels are a potential risk factor for stroke. |\n",
    "| **BMI vs. Stroke**           | No **strong separation** between stroke and non-stroke cases. | BMI alone may not be a strong predictor of stroke risk. |\n",
    "| **Age vs. Avg Glucose Level** | No clear linear trend but **some high glucose cases** are in older individuals. | Suggests a possible interaction effect between age and glucose level. |\n",
    "| **Age vs. BMI**              | BMI is **scattered across all ages**, but younger individuals show more variance. | No strong correlation, but BMI distributions differ slightly across age groups. |\n",
    "| **Avg Glucose Level vs. BMI** | Glucose levels are **widely spread** across BMI values. | Suggests **no strong direct correlation** between glucose levels and BMI. |\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Age is a strong differentiator** for stroke cases‚Äîhigher age groups have more stroke occurrences.\n",
    "- **High glucose levels** are more frequent in stroke cases.\n",
    "- **BMI does not show a strong direct correlation** with stroke but still contributes as a secondary factor.\n",
    "- **Possible interactions between age and glucose levels**‚Äîa more detailed analysis may be required to confirm relationships.\n",
    "\n",
    "This analysis aligns with previous statistical tests, confirming **age and glucose level as primary risk factors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbb15f-bda7-4299-8a87-9f2824014cdc",
   "metadata": {},
   "source": [
    "# **Point Biserial Correlation (Numerical Features vs. Binary Target)**\n",
    "## Since our target variable stroke is binary, we can compute Point Biserial Correlation to assess how strongly each numerical feature correlates with stroke.\n",
    "## Why? This can complement Pearson and Spearman correlations, as these are not ideal for binary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7e4ce-12a6-42ac-b410-774a967be8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Selecting numerical features\n",
    "numeric_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "# Computing Point Biserial Correlation for each numerical feature\n",
    "biserial_results = []\n",
    "for feature in numeric_features:\n",
    "    corr, p_value = pointbiserialr(df[feature], df['stroke'])\n",
    "    biserial_results.append({'Feature': feature, 'Correlation': corr, 'P-Value': p_value})\n",
    "\n",
    "# Creating a DataFrame to display results\n",
    "biserial_df = pd.DataFrame(biserial_results)\n",
    "\n",
    "# Display results\n",
    "print(f\"Point Biserial Correlation Results,{biserial_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32708c14-2922-491a-997e-166420e5268c",
   "metadata": {},
   "source": [
    "# *Key Insights:*\n",
    "## Age has the highest positive correlation (0.245) with stroke occurrence.\n",
    "### This aligns with previous findings that older individuals are more likely to experience strokes.\n",
    "### The extremely low p-value (7.03e-71) confirms statistical significance, meaning this relationship is not due to chance.\n",
    "## Average Glucose Level also shows a weak positive correlation (0.132) with stroke.\n",
    "### While weaker than age, this supports the idea that higher glucose levels may increase stroke risk.\n",
    "### The p-value (2.77e-21) confirms statistical significance.\n",
    "## BMI has missing correlation and p-value (NaN).\n",
    "### This could indicate a lack of variability in BMI values related to stroke occurrence.\n",
    "### This aligns with previous observations that BMI is a weaker predictor of stroke.\n",
    "\n",
    "## Conclusion:\n",
    "### Age is the strongest numerical predictor of stroke risk, reinforcing its importance in the model.\n",
    "### Glucose levels contribute to stroke prediction but with weaker strength.\n",
    "### BMI does not show a clear relationship, confirming previous analyses where BMI distributions overlapped significantly across stroke and non-stroke cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50323ec6-34a8-4d85-8a07-d6dbaf78e13c",
   "metadata": {},
   "source": [
    "# **Next Doing PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08474b73-361b-4896-8350-c88af8932ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_processed[['age', 'avg_glucose_level', 'bmi']]\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(pca_results, columns=['PC1', 'PC2'])\n",
    "df_pca['stroke'] = df_processed['stroke']\n",
    "\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='stroke', data=df_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9385d-7e98-49a9-92f6-534417737102",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis (PCA) Interpretation**\n",
    "## **Overview**\n",
    "### The plot represents the first two principal components (PC1 and PC2) of the dataset, showing a reduced-dimensional representation of the stroke classification problem.\n",
    "### Each point corresponds to an observation, colored by stroke status: Blue (0.0): Non-stroke cases Orange (1.0): Stroke cases\n",
    "## Key Observations\n",
    "### Majority of data points belong to the non-stroke category (blue), highlighting class imbalance.\n",
    "### This aligns with the dataset's known distribution, where stroke cases are significantly fewer.\n",
    "### Stroke cases (orange) appear sparsely distributed across the plot.\n",
    "### There is no clear, well-separated cluster for stroke cases.\n",
    "### This suggests that stroke vs. non-stroke classification may not be easily separable in a linear transformation of features.\n",
    "## **PC1 captures the majority of variation, while PC2 provides additional spread.**\n",
    "### The points are more spread along PC1, indicating it explains most of the variance in the data.\n",
    "### PC2 does not show strong separation between stroke and non-stroke cases, suggesting that a higher-dimensional transformation may be needed for better class distinction.\n",
    "## **Possible Overlap Between Stroke and Non-Stroke Groups**\n",
    "### **Stroke cases do not form a distinct cluster, implying that non-linear decision boundaries (such as those used in Random Forest or XGBoost) might perform better than purely linear models like Logistic Regression**.\n",
    "## **Conclusion**\n",
    "### PCA confirms that the **stroke classification problem is complex and non-linearly separable**.\n",
    "### Dimensionality reduction using PCA does not clearly separate stroke cases, suggesting that engineered features, non-linear transformations, or advanced models may be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa672c-48ff-43de-b219-f84c9ac7a5cd",
   "metadata": {},
   "source": [
    "# **Function To Find Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d827ba-b511-493d-a9a5-8e339c564bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df, target_column, model_type='xgboost'):\n",
    "    \"\"\"\n",
    "    Trains a model (XGBoost or Random Forest) and plots feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset with features and target.\n",
    "    target_column (str): The target variable (e.g., 'stroke').\n",
    "    model_type (str): 'xgboost' or 'random_forest' to choose the model.\n",
    "\n",
    "    Returns:\n",
    "    None (Displays feature importance plot).\n",
    "    \"\"\"\n",
    "\n",
    "    # Splitting features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Train the selected model\n",
    "    if model_type == 'xgboost':\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 'xgboost' or 'random_forest'.\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', hue = 'Feature',data=importance_df, legend = False)\n",
    "    plt.title(f'Feature Importance using {model_type.upper()}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9f592-dac3-4469-9146-8a9e31a2f29a",
   "metadata": {},
   "source": [
    "# **XGBOOST based feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66fbaf-e4e3-473a-8308-d41403f23e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(df_processed, target_column='stroke', model_type='xgboost') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d0cb3-1b4b-490e-9d45-478005be046c",
   "metadata": {},
   "source": [
    "# **RF based feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0194-cc37-4a37-96eb-4415f79c091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(df_processed, target_column='stroke', model_type='random_forest')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5914c-8a42-4ec9-afec-dccc9064a6db",
   "metadata": {},
   "source": [
    "# **Feature Importance Plot using SHAP Before using Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1617-3793-4474-ae35-f2bf7494f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_feature_importance(df, target_column):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model and plots SHAP feature importance.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset with features and target.\n",
    "    target_column (str): The target variable (e.g., 'stroke').\n",
    "\n",
    "    Returns:\n",
    "    None (Displays SHAP summary plot).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert boolean columns to integers\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=['bool']).columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "    # Splitting features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Train XGBoost model\n",
    "    model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Explain the model with SHAP\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    # Plot SHAP summary plot\n",
    "    shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9a4e8-cc02-4ca6-a607-ee06d3fc5ac4",
   "metadata": {},
   "source": [
    "# **SHAP based Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4d760-0461-41c3-9646-e72af6e6f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_feature_importance(df_processed, target_column='stroke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b4ca2-4cc5-420e-83a0-5534c1fbe791",
   "metadata": {},
   "source": [
    "# **Feature Importance Comparison Across Methods**\n",
    "\n",
    "| Rank | Feature          | XGBoost Importance | Random Forest Importance | SHAP Interpretation |\n",
    "|------|-----------------|---------------------|--------------------------|----------------------|\n",
    "| 1    | Age             | Highest            | High                     | Strong positive impact on stroke prediction |\n",
    "| 2    | Avg Glucose Level | Moderate          | Highest                  | High glucose levels contribute to stroke risk |\n",
    "| 3    | BMI             | Moderate           | High                     | Some impact, but weaker than age and glucose |\n",
    "| 4    | Heart Disease   | High               | Low                      | Important in XGBoost but not RF |\n",
    "| 5    | Smoking Status  | Moderate           | Moderate                 | Mixed impact across models |\n",
    "| 6    | Work Type       | Low                | Moderate                 | Shows importance in SHAP for some cases |\n",
    "| 7    | Hypertension    | Moderate           | Low                      | Contributes but weaker than age and glucose |\n",
    "| 8    | Ever Married    | Moderate           | Lowest                   | Age-related factor, weaker effect |\n",
    "| 9    | Gender         | Low                | Low                      | Not highly predictive across models |\n",
    "| 10   | Residence Type  | Lowest             | Low                      | Weak predictor of stroke |\n",
    "\n",
    "---\n",
    "\n",
    "# **Consistently Important Features Across All Three Methods**\n",
    "\n",
    "| Category      | Feature           | Why It's Important? |\n",
    "|--------------|------------------|---------------------|\n",
    "| Strongest Predictor | Age             | Stroke cases are concentrated in older age groups |\n",
    "| High Influence | Avg Glucose Level  | Elevated glucose levels indicate metabolic issues related to stroke |\n",
    "| Moderate Impact | BMI              | Some correlation, though weaker than age and glucose |\n",
    "| Contributing Factor | Smoking Status  | Smoking has known health risks, contributing to stroke prediction |\n",
    "| Secondary Factor | Heart Disease    | Strong impact in XGBoost, but lower in RF and SHAP |\n",
    "\n",
    "---\n",
    "\n",
    "# **Key Insights**\n",
    "\n",
    "## 1. **Age is the Most Important Factor**  \n",
    "###   - It is consistently ranked high in all three methods.\n",
    "###    - Older individuals show a significantly higher risk of stroke.\n",
    "\n",
    "## 2. **Avg Glucose Level is Highly Predictive**  \n",
    "###    - Random Forest considers it the most important feature.\n",
    "###    - XGBoost and SHAP also highlight its relevance.\n",
    "\n",
    "## 3. **BMI Has a Moderate Impact**  \n",
    "###    - Important in Random Forest but less prominent in XGBoost and SHAP.\n",
    "###    - Suggests it may play a secondary role in stroke prediction.\n",
    "\n",
    "## 4. **Heart Disease and Hypertension Are Not Always Strong Predictors**  \n",
    "###    - XGBoost assigns high importance to heart disease, but RF and SHAP do not.\n",
    "###    - This suggests potential feature interactions or dataset-specific effects.\n",
    "\n",
    "## 5. **Smoking Status Shows Some Influence**  \n",
    "###    - Ranked moderately in all models.\n",
    "###    - Could be interacting with other health-related factors.\n",
    "\n",
    "## 6. **Work Type, Gender, and Residence Type Have Low Predictive Power**  \n",
    "###    - These variables show limited influence across all three models.\n",
    "###    - Their contribution to stroke prediction is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dffe9-608b-47ce-96e4-8bf34919b20f",
   "metadata": {},
   "source": [
    "# **Is It a Problem That Importance Rankings Differ?**\n",
    "## No, it‚Äôs actually a strength. Different models provide complementary insights into stroke prediction.\n",
    "### If all models ranked features identically, it could indicate overfitting or bias.\n",
    "### Seeing variation means different algorithms capture different aspects of feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0316c-1805-4a3c-bbb1-cc58ca501083",
   "metadata": {},
   "source": [
    "# Doing Forward Feature Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03995e-4a79-4019-9003-36aef35965a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(df, model_name):\n",
    "    \"\"\"\n",
    "    Performs Forward Feature Selection (FFS) and plots F1-Score vs. Number of Features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with features and target ('stroke').\n",
    "    model_name (str): The model to use ('logistic' for Logistic Regression, 'xgb' for XGBoost, 'rf' for Random Forest).\n",
    "\n",
    "    Returns:\n",
    "    list: Selected features using Forward Feature Selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define features and target variable\n",
    "    X = df.drop(columns=['stroke'])  # Drop target variable\n",
    "    y = df['stroke']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Choose model based on user input\n",
    "    if model_name.lower() == 'logistic':\n",
    "        model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    elif model_name.lower() == 'xgb':\n",
    "        model = XGBClassifier(eval_metric=\"logloss\")\n",
    "    elif model_name.lower() == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_name. Choose 'logistic', 'xgb', or 'rf'.\")\n",
    "\n",
    "    # Initialize Forward Feature Selection\n",
    "    sfs = SFS(model, \n",
    "              k_features='best',  # Automatically selects the best number of features\n",
    "              forward=True, \n",
    "              floating=False, \n",
    "              scoring='f1',  # Use F1-score as evaluation metric\n",
    "              cv=5)\n",
    "\n",
    "    # Fit SFS\n",
    "    sfs.fit(X_train, y_train)\n",
    "\n",
    "    # Extract feature count and corresponding F1-scores\n",
    "    num_features = list(sfs.subsets_.keys())\n",
    "    f1_scores = [sfs.subsets_[k]['avg_score'] for k in num_features]\n",
    "\n",
    "    # Plot F1-score vs Number of Features\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(num_features, f1_scores, marker='o', linestyle='-', color='b', label='F1-Score')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title(f'F1-Score vs. Number of Selected Features ({model_name.upper()})')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Get final selected feature names\n",
    "    selected_features_ff = list(sfs.k_feature_names_)\n",
    "    print(f\"Selected Features using Forward Feature Selection ({model_name.upper()}):\", selected_features_ff)\n",
    "\n",
    "    return selected_features_ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba959a-a950-47a9-8e28-1291dd1a2d91",
   "metadata": {},
   "source": [
    "# FFA with LOGIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c04aa4-6bba-42cc-bc97-423b4e9bb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_logistic = forward_feature_selection(df_processed, model_name='logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d4ab3-be4e-4cc9-b5f5-01e499696330",
   "metadata": {},
   "source": [
    "# FFA with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cde29-6c30-4213-93cd-a960395bf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_xgb = forward_feature_selection(df_processed, model_name='xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3e2f9-dbe8-41fb-a04f-e976f528cdca",
   "metadata": {},
   "source": [
    "# FFA with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ded369-4953-4d6d-82b1-ffeced112ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_rf = forward_feature_selection(df_processed, model_name='rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0a1df-111e-41c0-8fe2-3c7c97133d90",
   "metadata": {},
   "source": [
    "# **Analysis of Forward Feature Addition (FFA) Results**\n",
    "## **Key Observations:**\n",
    "### **1. XGB and LOGIT show peak F1-score at 9 features**\n",
    "####   - Both **XGBoost and Logistic Regression** exhibit their best F1-score when using **9 features**.\n",
    "####   - This suggests that **9 features provide an optimal balance between model complexity and predictive performance**.\n",
    "####   - Beyond 9 features, the F1-score **declines or remains stable**, indicating diminishing returns from additional features.\n",
    "\n",
    "### **2. RF shows peak F1-score at only 2 features**\n",
    "####  - Unlike XGB and LOGIT, **Random Forest achieves its highest F1-score with only 2 features**.\n",
    "#### - Adding more features **decreases performance**, suggesting that RF is:\n",
    "####     - **Overfitting when too many features are included**.\n",
    "####     - **Finding only a few strong predictors, while additional features introduce noise**.\n",
    "####   - This behavior suggests that **RF's feature importance rankings may differ significantly from XGB and LOGIT**.\n",
    "\n",
    "\n",
    "## **Interpretation of Results:**\n",
    "### **1. Should RF be used with 9 features like XGB and LOGIT?**\n",
    "####   - **No**, because RF performs best with only **2 features**, meaning that additional features hurt its ability to generalize.\n",
    "####   - However, we should **analyze which 2 features RF selects** and whether they align with the features chosen by XGB and LOGIT.\n",
    "\n",
    "### **2. Should RFE be redone with only 2 features for RF?**\n",
    "####   - **Possibly, if RF is being considered for final model selection.**\n",
    "####   - However, since XGB and LOGIT perform best with **9 features**, using only 2 features for RF would make model comparisons inconsistent.\n",
    "####   - Instead, we can **compare RF's best 2-feature model against XGB & LOGIT‚Äôs 9-feature models** to see if RF still outperforms them with fewer variables.\n",
    "\n",
    "\n",
    "## **Final Decision on Features:**\n",
    "#### 1. **Use 9 features for XGB and LOGIT** (since F1-score peaks at 9).  We will check this by RFE\n",
    "#### 2. **Use 2 features for RF** (since it performs best with fewer variables). We will check this by RFE\n",
    "#### 3. **Compare performance between RF's 2-feature model and XGB/LOGIT's 9-feature models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf9185-0af1-4eee-ad26-55ad14eaf911",
   "metadata": {},
   "source": [
    "# **Balancing Response Variable by SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc89a2-11f5-43a6-ac1e-129b632eacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target variable\n",
    "X = df_processed.drop(columns=['stroke'])  # Drop target column\n",
    "y = df_processed['stroke']  # Target variable\n",
    "\n",
    "# Split into training and testing sets (SMOTE should only be applied to training data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the training set\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print class distribution before and after SMOTE\n",
    "print(\"Before SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts(normalize=True) * 100)\n",
    "\n",
    "# Now X_train_smote, y_train_smote, X_test, and y_test are ready for RFE or model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032642b-4cee-445e-8322-6955e3010f31",
   "metadata": {},
   "source": [
    "# **Recursive Feature Elimination By Logistic Regression. For interpretability And For Base Model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312a022-d580-44f2-b0d3-39be3e4c9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "# Lists to store F1-scores\n",
    "f1_scores = []\n",
    "num_features = []\n",
    "\n",
    "# Perform RFE iteratively\n",
    "for i in range(X_train_smote.shape[1], 0, -1):  # Start with all features, remove one at a time\n",
    "    rfe_logit = RFE(estimator=logreg, n_features_to_select=i)\n",
    "    rfe_logit.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Get feature subset\n",
    "    X_train_rfe = X_train_smote.loc[:, rfe_logit.support_]\n",
    "    X_test_rfe = X_test.loc[:, rfe_logit.support_]\n",
    "\n",
    "    # Train model on selected features\n",
    "    logreg.fit(X_train_rfe, y_train_smote)\n",
    "    y_pred = logreg.predict(X_test_rfe)\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    num_features.append(i)\n",
    "\n",
    "# Plot F1-score vs. Number of Features\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(num_features, f1_scores, marker='o', linestyle='-', color='b', label=\"F1-Score\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.title(\"F1-Score vs. Number of Selected Features (LOGIT - SMOTE)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Fit RFE for final selection with optimal number of features (e.g., 9)\n",
    "rfe_logit_final = RFE(estimator=logreg, n_features_to_select=9)\n",
    "rfe_logit_final.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_logit = X.columns[rfe_logit_final.support_].tolist()\n",
    "print(\"Selected Features using Logistic Regression RFE (SMOTE):\", selected_features_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cabe4f-152d-495e-958f-d8b0f64f5b8a",
   "metadata": {},
   "source": [
    "# **Recursive Feature Elimination By XGB. For performance And For (possibly) best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ab5fd-bde1-4993-afa9-9e21a10a7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(eval_metric=\"logloss\")\n",
    "\n",
    "# Lists to store F1-scores\n",
    "f1_scores = []\n",
    "num_features = []\n",
    "\n",
    "# Perform RFE iteratively\n",
    "for i in range(X_train_smote.shape[1], 0, -1):  \n",
    "    rfe_xgb = RFE(estimator=xgb_model, n_features_to_select=i)\n",
    "    rfe_xgb.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Get feature subset\n",
    "    X_train_rfe = X_train_smote.loc[:, rfe_xgb.support_]\n",
    "    X_test_rfe = X_test.loc[:, rfe_xgb.support_]\n",
    "\n",
    "    # Train model on selected features\n",
    "    xgb_model.fit(X_train_rfe, y_train_smote)\n",
    "    y_pred = xgb_model.predict(X_test_rfe)\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    num_features.append(i)\n",
    "\n",
    "# Plot F1-score vs. Number of Features\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(num_features, f1_scores, marker='o', linestyle='-', color='b', label=\"F1-Score\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.title(\"F1-Score vs. Number of Selected Features (XGB - SMOTE)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Fit RFE for final selection with optimal number of features (e.g., 9)\n",
    "rfe_xgb_final = RFE(estimator=xgb_model, n_features_to_select=9)\n",
    "rfe_xgb_final.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_xgb = X.columns[rfe_xgb_final.support_].tolist()\n",
    "print(\"Selected Features using XGBoost RFE (SMOTE):\", selected_features_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f18c0-ed2e-451d-9da0-bfe1931ee6f0",
   "metadata": {},
   "source": [
    "# **Recursive Feature Elimination By RF. For performance And For (possibly) best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24dee5-605d-4e9d-9ef8-0dc3a4e6f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Lists to store F1-scores\n",
    "f1_scores = []\n",
    "num_features = []\n",
    "\n",
    "# Perform RFE iteratively\n",
    "for i in range(X_train_smote.shape[1], 0, -1):  \n",
    "    rfe_rf = RFE(estimator=rf_model, n_features_to_select=i)\n",
    "    rfe_rf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Get feature subset\n",
    "    X_train_rfe = X_train_smote.loc[:, rfe_rf.support_]\n",
    "    X_test_rfe = X_test.loc[:, rfe_rf.support_]\n",
    "\n",
    "    # Train model on selected features\n",
    "    rf_model.fit(X_train_rfe, y_train_smote)\n",
    "    y_pred = rf_model.predict(X_test_rfe)\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    num_features.append(i)\n",
    "\n",
    "# Plot F1-score vs. Number of Features\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(num_features, f1_scores, marker='o', linestyle='-', color='b', label=\"F1-Score\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.title(\"F1-Score vs. Number of Selected Features (RF - SMOTE)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Fit RFE for final selection with optimal number of features (e.g., 2 for RF)\n",
    "rfe_rf_final = RFE(estimator=rf_model, n_features_to_select=2)\n",
    "rfe_rf_final.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_rf = X.columns[rfe_rf_final.support_].tolist()\n",
    "print(\"Selected Features using Random Forest RFE (SMOTE):\", selected_features_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abcc9f5-5e12-4e81-85e6-ea41dd4521e0",
   "metadata": {},
   "source": [
    "# **Comparison of Recursive Feature Elimination (RFE) Results After SMOTE**\n",
    "\n",
    "## **1Ô∏è‚É£ Logistic Regression (LOGIT)**\n",
    "#### - **F1-score remains low until 8-10 features are included.**\n",
    "#### - A **sharp increase in F1-score after 8 features** suggests that the model needs more features to achieve reasonable performance.\n",
    "#### - **Selected Features:**  \n",
    "####   `['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'bmi', 'smoking_status']`\n",
    "\n",
    "\n",
    "## **2Ô∏è‚É£ XGBoost (XGB)**\n",
    "#### - **F1-score peaks early (2-3 features) and drops later.**\n",
    "#### - **Selected Features:**  \n",
    "  `['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'smoking_status']`\n",
    "#### - **Key Differences from LOGIT:**  \n",
    "####   - `avg_glucose_level` is selected instead of `bmi`.\n",
    "####   - Feature ranking fluctuates, indicating that **XGBoost may be more sensitive to certain interactions**.\n",
    "\n",
    "## **3Ô∏è‚É£ Random Forest (RF)**\n",
    "#### - **Performs best with just 2 features: `age` and `avg_glucose_level`**\n",
    "#### - This confirms **RF relies on fewer but highly predictive features**.\n",
    "#### - **Selected Features:**  \n",
    "####   `['age', 'avg_glucose_level']`\n",
    "\n",
    "## **Key Takeaways**\n",
    "### 1. **SMOTE changed feature selection for all models.**\n",
    "####    - **Before SMOTE**, RF selected `avg_glucose_level` and `bmi` as the top 2 features.  \n",
    "####    - **After SMOTE**, RF prefers `age` and `avg_glucose_level`.\n",
    "\n",
    "### 2. **Logistic Regression and XGB favor more features (9-10), while RF needs only 2.**\n",
    "####    - This means **ensemble models like XGB/RF might perform better with feature pruning**.\n",
    "####    - **Logistic Regression requires a larger set of features** for reasonable classification.\n",
    "\n",
    "### 3. **Finalized Feature Set for Model Training**\n",
    "####    - **For LOGIT & XGB:** Use **9 features**  \n",
    "####      `['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'bmi', 'smoking_status']`\n",
    "####    - **For RF:** Use **only 2 features**  \n",
    "####      `['age', 'avg_glucose_level']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd4276-0ad3-4381-8c8d-9e0a179768c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, classification_report, precision_recall_curve, f1_score\n",
    ")\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Apply SMOTE Before Train-Test Split\n",
    "X = df_processed.drop(columns=['stroke'])  # Drop target variable\n",
    "y = df_processed['stroke']  # Target variable\n",
    "\n",
    "# Apply SMOTE before splitting\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the balanced dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Step 2: Define Models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Step 3: Define Hyperparameter Grid\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'saga', 'liblinear', 'newton-cg'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 4: Function to Train and Evaluate Models\n",
    "def train_and_evaluate_model(model_name, selected_features):\n",
    "    print(f\"\\n Training {model_name}...\")\n",
    "\n",
    "    # Select the best features for the model\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # Get the model and hyperparameter grid\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Hyperparameter Tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Get Best Model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\" Best Parameters for {model_name}: {best_params}\")\n",
    "\n",
    "    # Predict Probabilities\n",
    "    y_scores = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # Compute AUC-ROC\n",
    "    auc_roc = roc_auc_score(y_test, y_scores)\n",
    "    print(f\" {model_name} - AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    # Optimize Decision Threshold Using Precision-Recall Curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "    # Apply Best Threshold\n",
    "    y_pred = (y_scores >= best_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate Performance\n",
    "    print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the model\n",
    "    model_path = f'./models/best_{model_name}.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 5: Define Best Features for Each Model\n",
    "selected_features_logit = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                           'work_type', 'Residence_type', 'bmi', 'smoking_status']\n",
    "selected_features_xgb = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                         'work_type', 'Residence_type', 'avg_glucose_level', 'smoking_status']\n",
    "selected_features_rf = ['age', 'avg_glucose_level']  # RF only needs 2 features\n",
    "\n",
    "# Step 6: Train & Evaluate All Models\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "best_logreg = train_and_evaluate_model('LogisticRegression', selected_features_logit)\n",
    "best_rf = train_and_evaluate_model('RandomForest', selected_features_rf)\n",
    "best_xgb = train_and_evaluate_model('XGBoost', selected_features_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e847b-07e4-4e23-8888-9e7c8dca177f",
   "metadata": {},
   "source": [
    "# **ABOVE RESULTS ARE WITH DEFAULT THREHOLD (0.5) FOR EACH MODEL. FOR EACH BEST MODEL, WE FIND WHAT THRESHOLD (INSTEAD OF DEFAULT 0.5) GIVES THE BEST POSSIBLE F-1 SCORE  FOR EACH MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b59f3d-c895-4bb4-aeae-d12c22478989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best models\n",
    "logistic_model = joblib.load(\"./models/best_LogisticRegression.pkl\")\n",
    "rf_model = joblib.load(\"./models/best_RandomForest.pkl\")\n",
    "xgb_model = joblib.load(\"./models/best_XGBoost.pkl\")\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "# Define the best-selected features for each model\n",
    "selected_features_logit = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                           'work_type', 'Residence_type', 'bmi', 'smoking_status']\n",
    "selected_features_xgb = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                         'work_type', 'Residence_type', 'avg_glucose_level', 'smoking_status']\n",
    "selected_features_rf = ['age', 'avg_glucose_level']  # RF only needs 2 features\n",
    "\n",
    "# Models dictionary with their respective selected features\n",
    "models = {\n",
    "    \"Logistic Regression\": (logistic_model, selected_features_logit),\n",
    "    \"Random Forest\": (rf_model, selected_features_rf),\n",
    "    \"XGBoost\": (xgb_model, selected_features_xgb)\n",
    "}\n",
    "\n",
    "# Generate Precision-Recall curves and find optimal F1-score thresholds\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, (model_name, (model, selected_features)) in enumerate(models.items(), 1):\n",
    "    # Select correct features for the test set\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_scores = model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # Compute Precision-Recall curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "    # Compute F1 scores for each threshold\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "\n",
    "    # Find best F1-score threshold\n",
    "    best_f1_index = np.argmax(f1_scores)\n",
    "    best_f1_threshold = thresholds[best_f1_index] if best_f1_index < len(thresholds) else 0.5\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        \"Optimal Threshold (F1)\": round(best_f1_threshold, 4),\n",
    "        \"Best F1-Score\": round(f1_scores[best_f1_index], 4),\n",
    "    }\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.plot(recalls, precisions, label=\"Precision-Recall Curve\", linewidth=2)\n",
    "    plt.scatter(recalls[best_f1_index], precisions[best_f1_index], color='red', marker='o',\n",
    "                label=f'Best F1 ({best_f1_threshold:.2f})')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{model_name} Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e104e5f-bf18-4b05-a880-fb2a0aaf560f",
   "metadata": {},
   "source": [
    "| Model | Optimal Threshold (F1) | Best F1-Score |\n",
    "|--------|------------------|------------|\n",
    "| Logistic Regression | 0.3087 | 0.8259 |\n",
    "| Random Forest | 0.4997 | 0.9106 |\n",
    "| XGBoost | 0.5091 | 0.9531 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1119377-13fb-4ebd-9c68-59a1337a2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing classification reports again for these optimized thresholds: \n",
    "# Step 1: Apply SMOTE Before Train-Test Split\n",
    "X = df_processed.drop(columns=['stroke'])  # Drop target variable\n",
    "y = df_processed['stroke']  # Target variable\n",
    "\n",
    "# Apply SMOTE before splitting\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the balanced dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Step 2: Define Models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Step 3: Define Hyperparameter Grid\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'saga', 'liblinear', 'newton-cg'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 4: Optimal Thresholds for Best F1-Score\n",
    "optimal_thresholds = {\n",
    "    'LogisticRegression': 0.3087,\n",
    "    'RandomForest': 0.4997,\n",
    "    'XGBoost': 0.5091\n",
    "}\n",
    "\n",
    "# Step 5: Function to Train and Evaluate Models with Optimal Threshold\n",
    "def train_and_evaluate_model(model_name, selected_features):\n",
    "    print(f\"\\n Training {model_name}...\")\n",
    "\n",
    "    # Select the best features for the model\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # Get the model and hyperparameter grid\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Hyperparameter Tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Get Best Model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\" Best Parameters for {model_name}: {best_params}\")\n",
    "\n",
    "    # Predict Probabilities\n",
    "    y_scores = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # Compute AUC-ROC\n",
    "    auc_roc = roc_auc_score(y_test, y_scores)\n",
    "    print(f\" {model_name} - AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    # Apply Best Threshold from Earlier Analysis\n",
    "    best_threshold = optimal_thresholds[model_name]\n",
    "    y_pred = (y_scores >= best_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate Performance\n",
    "    print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the model\n",
    "    model_path = f'./Optimized_Threshold_models/best_{model_name}.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 6: Define Best Features for Each Model\n",
    "selected_features_logit = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                           'work_type', 'Residence_type', 'bmi', 'smoking_status']\n",
    "selected_features_xgb = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                         'work_type', 'Residence_type', 'avg_glucose_level', 'smoking_status']\n",
    "selected_features_rf = ['age', 'avg_glucose_level']  # RF only needs 2 features\n",
    "\n",
    "# Step 7: Train & Evaluate All Models with Optimal Thresholds\n",
    "os.makedirs(\"./Optimized_Threshold_models\", exist_ok=True)\n",
    "\n",
    "best_logreg = train_and_evaluate_model('LogisticRegression', selected_features_logit)\n",
    "best_rf = train_and_evaluate_model('RandomForest', selected_features_rf)\n",
    "best_xgb = train_and_evaluate_model('XGBoost', selected_features_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1c42e-ff6e-468c-9fb0-d7bf0177b9f6",
   "metadata": {},
   "source": [
    "# **Our results indicate no significant difference in the classification metrics when using the optimized F1-score thresholds versus the default 0.5 threshold.**\n",
    "# This suggests that the default threshold of 0.5 was already close to optimal for your models. and **our models are well-calibrated** and handle class imbalance effectively.\n",
    "#  **Which Threshold Should You Use?**\n",
    "# If we Want to Prioritize Balance (General Performance), we stick to the default threshold of 0.5.\n",
    "# If we Want to Optimize for Higher Recall (Capture More Stroke Cases) , we use the optimized F1-score thresholds. This would slightly increase the number of detected stroke cases (True Positives) but might increase False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc8698-c09d-4fc1-a230-0c4b89277110",
   "metadata": {},
   "source": [
    "# **Obtain AUC ROC Curve for default threshold (0.5) models and optimized threshold (0.5091) models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305b4ec-307b-44ba-bfd4-30ec038d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load best models with optimized thresholds\n",
    "optimized_logistic = joblib.load(\"./Optimized_Threshold_models/best_LogisticRegression.pkl\")\n",
    "optimized_rf = joblib.load(\"./Optimized_Threshold_models/best_RandomForest.pkl\")\n",
    "optimized_xgb = joblib.load(\"./Optimized_Threshold_models/best_XGBoost.pkl\")\n",
    "\n",
    "#  Load best models with default thresholds\n",
    "default_logistic = joblib.load(\"./models/best_LogisticRegression.pkl\")\n",
    "default_rf = joblib.load(\"./models/best_RandomForest.pkl\")\n",
    "default_xgb = joblib.load(\"./models/best_xgboost.pkl\")\n",
    "\n",
    "#  Define models with selected features\n",
    "optimized_models = {\n",
    "    \"Logistic Regression (Optimized)\": (optimized_logistic, selected_features_logit),\n",
    "    \"Random Forest (Optimized)\": (optimized_rf, selected_features_rf),\n",
    "    \"XGBoost (Optimized)\": (optimized_xgb, selected_features_xgb)\n",
    "}\n",
    "\n",
    "default_models = {\n",
    "    \"Logistic Regression (Default)\": (default_logistic, selected_features_logit),\n",
    "    \"Random Forest (Default)\": (default_rf, selected_features_rf),\n",
    "    \"XGBoost (Default)\": (default_xgb, selected_features_xgb)\n",
    "}\n",
    "\n",
    "#  Function to Plot AUC-ROC Curves\n",
    "def plot_auc_roc(models, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for model_name, (model, selected_features) in models.items():\n",
    "        X_test_selected = X_test[selected_features]  # Select features\n",
    "\n",
    "        # Get probability scores\n",
    "        y_scores = model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "        # Compute ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.4f})\")\n",
    "\n",
    "    #  Plot formatting\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Diagonal line for random classifier\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#  Plot AUC-ROC for both optimized and default models\n",
    "plot_auc_roc(optimized_models, \"AUC-ROC Curves for Optimized Models\")\n",
    "plot_auc_roc(default_models, \"AUC-ROC Curves for Default Models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424341c-86c4-45d7-9a6e-51791621757c",
   "metadata": {},
   "source": [
    "# **Why We Prioritize Maximizing Recall While Maintaining a Good F1-Score in Stroke Probability Prediction?**\n",
    "## In stroke probability prediction, the **primary goal is to minimize false negatives, i.e., cases where a person actually has a high risk of stroke but is misclassified as low risk**. This is critical because:\n",
    "## High Recall Reduces Risk of Missing Stroke Cases\n",
    "### A false negative (FN) means a high-risk patient is not flagged for further medical attention.\n",
    "### Missing a potential stroke patient can be life-threatening, as strokes require immediate medical intervention.\n",
    "## Stroke is a Critical Medical Condition\n",
    "### Early detection allows preventive measures (lifestyle changes, medication, or medical procedures).\n",
    "### **The consequences of a missed stroke prediction are far more severe than falsely flagging a healthy individual (false positive).**\n",
    "## F1-Score Ensures a Balance Between Precision & Recall\n",
    "### While recall is crucial, very high recall alone may lower precision (many false positives).\n",
    "### A good F1-score (harmonic mean of precision & recall) ensures we do not excessively misclassify healthy individuals.\n",
    "### This is important because too many false alarms could lead to unnecessary tests, stress, and medical costs.\n",
    "## **Medical Decision-Making Favors a Conservative Approach**\n",
    "### In medical diagnostics, it‚Äôs always better to be cautious than to miss a critical case.\n",
    "### **If the model flags some low-risk individuals as high-risk (false positives), they can still undergo further screening.**\n",
    "### But if high-risk individuals are missed, they lose the chance for early intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92326662-03fe-484d-a11e-4e735c401746",
   "metadata": {},
   "source": [
    "# Final Conclusion\n",
    "\n",
    "## For stroke prediction, recall is the priority because missing a stroke-prone patient can have fatal consequences.\n",
    "## However, we balance recall with a good F1-score to avoid excessive false positives and unnecessary interventions.\n",
    "## Threshold tuning (as seen in our 0.5091 optimal threshold) helps us find the best trade-off between recall and precision.\n",
    "## Thus, our stroke probability model is optimized for high recall (0.96) while keeping an F1-score of 0.95, ensuring both high sensitivity and balanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a9000-8a64-4cb3-b31a-c1a1bcbac49e",
   "metadata": {},
   "source": [
    "# **Obtain Predicted Probability Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35ddfb-023e-4aa0-a5b1-a2d485954bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best XGBoost model\n",
    "xgb_model = joblib.load(\"./Optimized_Threshold_models/best_XGBoost.pkl\")\n",
    "\n",
    "#  Get probability scores\n",
    "y_scores = xgb_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Plot the histogram of probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_scores, bins=50, kde=True, color='blue', edgecolor='black')\n",
    "plt.axvline(0.5, color='red', linestyle='--', label='Threshold = 0.5')\n",
    "plt.axvline(0.5091, color='green', linestyle='--', label='Optimized Threshold = 0.5091')\n",
    "plt.xlabel(\"Predicted Probability of Stroke\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Distribution of Predicted Probabilities (XGBoost)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#  Print summary statistics\n",
    "print(f\"Mean Probability: {np.mean(y_scores):.4f}\")\n",
    "print(f\"Median Probability: {np.median(y_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(y_scores):.4f}\")\n",
    "print(f\"Percentage of samples in [0.4, 0.6] range: {np.mean((y_scores >= 0.4) & (y_scores <= 0.6)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b481c-ab44-4bd5-9214-74167117ddae",
   "metadata": {},
   "source": [
    "# **Interpretations Of above probability distributions**\n",
    "### **Most Predictions are Extremely Confident**\n",
    "#### The histogram shows two strong peaks at 0 and 1, meaning the model is classifying most samples with high certainty.\n",
    "#### Very few predictions fall around the decision boundary (0.4 - 0.6 range is only 3.34%).\n",
    "#### This explains why threshold tuning doesn't impact results much‚Äîthere aren‚Äôt enough ambiguous predictions near the threshold.\n",
    "###  The Optimized Threshold (0.5091) is Close to 0.5\n",
    "#### The mean probability is 0.5084, very close to the default threshold of 0.5.\n",
    "#### **The median probability is 0.5557, slightly shifted towards 1, indicating more samples predicted as stroke.**\n",
    "#### This tiny shift from 0.5 to 0.5091 has minimal impact on classification.\n",
    "### Standard Deviation Shows a Wide Spread\n",
    "#### The standard deviation of 0.4551 suggests a high variance in prediction confidence.\n",
    "#### Most samples are confidently classified (either close to 0 or close to 1), meaning relabeling with a new threshold won‚Äôt change many predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77624db9-8f16-442c-9166-50c82765071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbcb2a-e5c2-47be-8274-c34656a88a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a963c-0b56-424c-815e-828ea233ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions for observations 4937 and 7032\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# Load the model\n",
    "model_path = 'C:\\\\Users\\\\Ecube\\\\sandiego\\\\project\\\\Optimized_Threshold_models\\\\best_XGBoost.pkl'\n",
    "model = load(model_path)\n",
    "\n",
    "# Assuming X_test_selected is already loaded in your session, if not, load it appropriately\n",
    "# For example: X_test_selected = pd.read_csv('path_to_X_test_selected.csv')\n",
    "\n",
    "# Select specific observations\n",
    "observations = X_test_selected.loc[[4937, 7032]]\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict_proba(observations)[:, 1]  # Get the probability of 'stroke'\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results = pd.DataFrame({\n",
    "    'Index': [4937, 7032],\n",
    "    'Predicted Probability': probabilities\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b15270-f3e3-4613-9b73-922834941d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 1: Load the saved XGBoost model\n",
    "xgb_model = joblib.load(\"./Optimized_Threshold_models/best_XGBoost.pkl\")\n",
    "\n",
    "#  Step 2: Load the new patient data\n",
    "new_patients = X_test_selected.copy()\n",
    "\n",
    "#  Step 3: Ensure column order matches training\n",
    "selected_features_xgb = [\n",
    "    \"gender\", \"age\", \"hypertension\", \"heart_disease\", \"ever_married\", \n",
    "    \"work_type\", \"Residence_type\", \"avg_glucose_level\", \"smoking_status\"\n",
    "]\n",
    "\n",
    "#  Step 4: Make probability predictions\n",
    "stroke_probabilities = xgb_model.predict_proba(new_patients[selected_features_xgb])[:, 1]  # Prob of stroke (class 1)\n",
    "\n",
    "#  Step 5: Apply optimized threshold (0.5091) to create binary predictions\n",
    "optimal_threshold = 0.5091\n",
    "stroke_predictions = (stroke_probabilities >= optimal_threshold).astype(int)\n",
    "\n",
    "#  Step 6: Add results to the dataframe\n",
    "new_patients[\"Stroke_Probability\"] = stroke_probabilities\n",
    "new_patients[\"Stroke_Prediction\"] = stroke_predictions  # 0 or 1 based on threshold\n",
    "\n",
    "#  Step 7: Save predictions with 4 decimal places\n",
    "new_patients.to_csv(\"batch_predictions.csv\", index=False, float_format=\"%.4f\")\n",
    "\n",
    "print(\" Batch predictions saved as batch_predictions.csv with 4 decimal precision.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9be5d9-5254-46f9-9e83-ccd1cfd83f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions file\n",
    "predictions = pd.read_csv(\"batch_predictions.csv\")\n",
    "\n",
    "# Load actual labels (ensure order is correct)\n",
    "y_test = y_test.reset_index(drop=True)  # Ensure indices match\n",
    "\n",
    "\n",
    "\n",
    "# Add actual stroke labels for comparison\n",
    "predictions[\"actual_stroke\"] = y_test.values  # Ensure order matches\n",
    "\n",
    "# Save the final comparison\n",
    "predictions.to_csv(\"predictions_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Final comparison saved as predictions_comparison.csv\")\n"
   ]
  },
  {
   "attachments": {
    "b784e215-2f6f-479e-9c81-986ae84c9a9a.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d7df2b6c-a856-43f0-8410-36c9ee9d5484",
   "metadata": {},
   "source": [
    "![image.png](attachment:b784e215-2f6f-479e-9c81-986ae84c9a9a.png)\n",
    "\n",
    "## **Thus predicted stroke and the actual value of 'stroke' in X_test closely match. This means our model performs well in batch prediction and it is ready to be put in production** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74d3d8-4703-4e3d-b115-27ee3aeb9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze>> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b016a-ff37-411f-951c-171f509f0766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py310projectvenv",
   "language": "python",
   "name": "py310projectvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
